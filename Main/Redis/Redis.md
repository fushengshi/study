# Redis

## 持久化

持久化方案选择：

- Redis 保存的数据丢失一些没什么影响，可以选择使用 RDB。
- 不建议单独使用 AOF，因为时不时地创建一个 RDB 快照可以进行数据库备份、更快的重启以及解决 AOF 引擎错误。
- 如果保存的数据要求安全性比较高的话，建议同时开启 RDB 和 AOF 持久化或者开启 RDB 和 AOF 混合持久化。

### AOF 日志

AOF（Append Only File）持久化，保存写操作命令到日志。Redis 每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里，Redis 重启的时候，先读取这个文件里的命令并执行。

#### 刷盘策略

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240529230300448.png" alt="image-20240529230300448" style="zoom:67%;" />

- Redis **执行完写操作命令后**，会将命令追加到  `server.aof_buf` 缓冲区。

  - 数据可能会丢失

    **执行写操作命令和记录日志是两个过程**，当 Redis 在还没来得及将命令写入到硬盘服务器发生宕机，这个数据就会有丢失的风险。

  - 可能阻塞其他操作

    虽然 AOF 是写后日志，避免阻塞当前命令的执行，但 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候会阻塞后续的操作。

  > Redis 的 AOF 日志的记录顺序与传统关系型数据库相反，Redis 先执行命令把数据写入内存，然后再记录日志到文件。
  >
  > 通常情况下，关系型数据库（如 MySQL）的日志都是 WAL（Write Ahead Log），在实际写数据之前，先把修改的数据记到日志文件中，以便当出现故障时进行恢复。

- 通过 `write()` 系统调用，将 `server.aof_buf` 缓冲区的数据写入到 AOF 文件的内核缓冲区 Page Cache。

  此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 Page Cache，内核缓冲区的数据根据 `appendfsync` 参数通过 `fsync()` 写入到硬盘：

  - everysec（默认）

    每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，创建一个异步任务来执行 `fsync()` 函数，**每隔一秒**将缓冲区里的内容写回到硬盘。

    避免了 always 策略的性能开销，比 no 策略更能避免数据丢失，当然如果上一秒的写操作命令日志没有写回到硬盘，发生了宕机，这一秒内的数据自然也会丢失。

  - always

    每次写操作命令执行完后，马上执行 ` fsync() ` 函数同步将 AOF 日志数据写回硬盘。

    可以最大程度保证数据不丢失，影响主进程的性能。

    > 无法避免因为执行写操作命令和记录日志是两个过程，执行写操作命令后宕机的数据丢失。

  - no

    每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，不执行 `fsync() ` 函数，由操作系统决定何时将缓冲区内容写回硬盘。

    操作系统写回硬盘的时机是不可预知的，如果 AOF 日志内容没有写回硬盘，一旦服务器宕机，就会丢失不定数量的数据。



#### AOF 重写

当 AOF 文件的大小超过所设定的阈值后，Redis 会启用 AOF 重写机制压缩 AOF 文件。读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到新的 AOF 文件，等到全部记录完后，将新的 AOF 文件替换旧的 AOF 文件。

Redis 会将新的写操作放在重写缓存区中，等待 AOF 重写操作完成的时候，将新操作直接追加到新的 AOF 中。

- `fork` 出一条子线程来将文件重写，在执行 `BGREWRITEAOF` 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子线程创建新 AOF 文件期间，记录服务器执行的所有写命令。

- 当子线程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。

  > 将缓冲区中新数据写到新文件的过程中会产生阻塞。

- 服务器用新的 AOF 文件替换旧的 AOF 文件，完成 AOF 文件重写操作。

`no-appendfsync-on-rewrite` 策略：

- no

  在 AOF 日志重写时，刷盘会被阻塞。如果当前 AOF 文件很大，相应的 AOF 重写时间会变长，刷盘被阻塞的时间也会更长。

- yes

  在 AOF 日志重写时，不进行刷盘操作，而只是将命令放在重写缓冲区里，避免磁盘 I/O 冲突。

  重写期间**有数据丢失的风险**。



### RDB 快照（默认）

RDB（Redis Database）快照就是记录某一个瞬间的内存数据，记录的是实际数据。Redis 恢复数据时直接将 RDB 文件读入内存，不需要像 AOF 额外执行操作命令的步骤，恢复数据的效率比 AOF 高。

> AOF 文件记录的是命令操作的日志，不是实际的数据。

Redis 默认采用的持久化方式，在 `redis.conf` 配置文件中默认有此下配置：

```shell
save 900 1      # 在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发bgsave命令创建快照。
save 300 10     # 在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发bgsave命令创建快照。
save 60 10000   # 在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发bgsave命令创建快照。
```

Redis 的快照是全量快照，每次执行快照把内存中的所有数据都记录到磁盘中，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。

#### 数据一致性

执行 `bgsave` 命令，创建一个**子进程**生成 RDB 文件，可以避免主线程的阻塞。执行 `bgsave` 命令的时候，会通过 `fork` 创建子进程。创建子进程的时候会复制父进程的页表，页表指向的物理内存相同，所以子进程和父进程共享同一片内存数据。

如果主线程（父进程）要修改共享数据里的某一块数据时，就会发生写时复制（Copy-On-Write，COW）。

- 数据的物理内存会被复制一份，主线程在这个数据副本进行修改操作。

- `bgsave` 子进程可以继续把原来的数据写入到 RDB 文件。

  发生了写时复制后，RDB 快照保存的是原本的内存数据，而主线程刚修改的数据没办法在此时写入 RDB 文件，只能交由下一次的 `bgsave` 快照。



### 混合持久化

混合持久化默认关闭，可以通过配置项 `aof-use-rdb-preamble` 开启。

混合持久化工作在 **AOF 日志重写过程**，AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。

- 在 AOF 重写日志时，`fork` 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件。
- 主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，
- 写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。



## 高可用

### 主从复制

![image-20240520001948845](https://gitee.com/fushengshi/image/raw/master/image-20240520001948845.png)

![image-20240527023721382](https://gitee.com/fushengshi/image/raw/master/image-20240527023721382.png)

Redis 提供了主从复制模式，Redis 主节点每次收到**写命令**之后，先写到内部的缓冲区，然后异步发送给从节点。

主从复制共有三种模式：

- 全量复制

  Master 收到 `psync` 命令后，会执行 `bgsave` 命令，`fork` 出一个**子进程**。

  - 子进程中将所有的数据按特定编码存储到 RDB 文件中。
  - RDB 生成完成之后就会把文件发送给从库，从库接收到 RDB 文件后，先清空当前数据库数据，然后加载 RDB 文件。

- 基于⻓连接的命令传播

  当主从库完成了全量复制，它们之间就会⼀直维护⼀个⽹络连接，主库会通过这个连接将后续收到的命令操作同步给从库。

- 增量复制

  第一次全量复制后，主从之间使用长连接进行命令传播，如果网络出现闪断（断了一会又恢复了）用户可能从 Salve 读到旧数据。

  在网络短暂断开后，Salve 重新连上 Master 时，Salve 会通过 `psync` 命令将自己的复制偏移量 Offset 发送给 Master，Master 根据自己的 Offset 和 Slave 的 Offset 之间的差距决定对 Salve 执行的同步方式：

- 判断出 Salve 要读取的数据还在 `repl_backlog_buffer` 里，主服务器采用增量同步的方式。
  - 判断出读取的数据已经不存在 `repl_backlog_buffer` 里（比如被覆盖掉了），主服务器采用全量同步的方式。

  > 复制积压缓冲区 （replication backlog buffer，repl_backlog_buffer）
  >
  > 复制积压缓冲区是由 Master 维护的一个固定长度的环形缓冲区、是先进先出（FIFO）队列，默认大小为 1 MB。当主服务器进行命令传播时，它不仅会将写命令发送给所有从服务器，还会将写命令入队到复制积压缓冲区里面。

![image-20240713180829599](https://gitee.com/fushengshi/image/raw/master/image-20240713180829599.png)



#### 数据一致性

全量复制生成 RDB 的是通过 `fork` 出的子进程做的，主进程可以正常处理写入命令。

为保证主从的数据一致性 Master 会用 `replication buffer` 记录 RDB 文件生成、加载 RDB 文件期间收到的所有写操作。Master 会把记录在 `replication buffer` 的写命令发送给 Salve，实现主从同步一致性。



### 哨兵机制

哨兵（Sentinel）机制作用是实现主从节点故障转移。它会监测主节点是否存活，如果发现主节点挂了，就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端。

> 主从复制和 Redis Sentinel 这两种⽅案本质都是通过增加主库的副本数量的⽅式来提⾼ Redis 服务的整体可⽤性和读吞吐量，都不⽀持横向扩展来缓解写压⼒以及解决缓存数据量过⼤的问题（解决方案见 Redis Cluster）。

#### 哨兵 Leader 选举

哨兵集群中选出一个哨兵 Leader 执行主从切换，哨兵 Leader 选举是基于 **Raft 算法**实现的。

- 哨兵集群中的每个哨兵节点只有在自己判定主库下线时，会给自己投票，其他的哨兵节点会把票投给第一个来要票的请求，其后的请求都会被拒绝。
- 如果出现多个哨兵节点同时发现主库下线并给自己投票，导致投票选举失败，就会触发新一轮投票直至成功。
- 选举过程遵循少数服从多数的原则，不同的哨兵节点必须获得大多数成员的同意才能被选为 Leader。

> 对比 MySQL 主从选举机制。

#### 主从切换（故障转移）

当大部分哨兵节点同意某个主节点死亡时，会宣布此主已死亡，并在所有哨兵之间广播这一消息，哨兵 Leader 会开始故障转移的工作。

- 在已下线主节点（旧主节点）属下的所有从节点里面，挑选出一个从节点，并将其转换为主节点，选择的规则：
  - 过滤掉已经离线的从节点。

  - 过滤掉历史网络连接状态差的从节点（通过 down-after-milliseconds）。

  - 将剩下的从节点，进行三轮考察：**优先级、复制进度、ID 号**。

    在每一轮考察过程中，如果找到了一个胜出的从节点，就将其作为新主节点。
- 让已下线主节点属下的所有从节点修改复制目标，修改为复制新主节点。
- 将新主节点的 IP 地址和信息，通过发布者 / 订阅者机制通知给客户端。
- 继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点。

#### 主从切换问题

- 异步复制同步丢失

  因为主从节点间的命令复制是异步进行的，所以无法实现强一致性保证。

  - 配置 `min-slaves-max-lag` 参数

    一旦所有的从节点数据复制和同步的延迟都超过了该值，主节点拒绝接收任何请求。

    将 Master 和 Slave 数据差控制在该值，Master 宕机丢失未复制的该值数据。

  - 客户端发现 Master 不可写后采取降级措施

    比如将数据暂时写入本地缓存和磁盘中、将数据写入 kafka 消息队列。

- 脑裂导致数据丢失

  由于网络问题集群节点之间失去联系。主从数据不同步重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，导致之前客户端写入的数据丢失。

  - 配置 `min-slaves-max-lag` 参数

    一旦所有的从节点数据复制和同步的延迟都超过了该值，主节点拒绝接收任何请求。

  - 配置 `min-slaves-to-write` 参数

    主节点必须要有至少该值个从节点连接，如果小于这个数，主节点会禁止写数据。

  脑裂无法彻底解决：

  例如 `min-slaves-to-write` 设置为 1，`min-slaves-max-lag` 设置为 15s，哨兵判断主节点客观下线的限制 `down-after-milliseconds` 为 10s，哨兵主从切换需要 5s。

  主节点卡住 12s 这时哨兵集群判断主节点下线，同时哨兵集群做主从切换需要 5s，这就意味着主从切换过程中，主节点恢复运行，而 `min-slaves-max-lag` 设置为 15s 那么主节点还是可写，也就是说在 12s~15s 这期间如果有客户端写入原主节点，那么这段时间的数据会丢失。

  > 脑裂本质是 Redis 主从集群内部没有通过共识算法，来维护多个节点数据的强一致性。
  >
  > Zookeeper 每次写请求超过半数节点写成功后才认为成功，当脑裂发生时 Zookeeper 主节点无法写入超过半数节点，写请求会直接返回失败，因此可以保证集群数据的一致性。



### Redis Cluster

Redis Cluster 是一种分布式去中心化的运行模式，是在 Redis 3.0 版本中推出的 Redis 集群方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。

- 可以横向扩展缓解写压⼒和存储压⼒，⽀持动态扩容和缩容。

- 具备主从复制、故障转移（内置了 Sentinel 机制，⽆需单独部署 Sentinel 集群）等开箱即⽤的功能。

Redis Cluster 是去中⼼化的（各个节点基于 Gossip 进⾏共享信息），任何⼀个 Master 出现故障，其它的 Master 节点不受影响，因为 key 找的是哈希槽⽽不是 Redis 节点。如果宕机的 Master ⽆ Slave 的话，为了保障集群的完整性，保证所有的哈希槽都指派给了可⽤的Master ，整个集群将不可⽤。

#### 哈希槽

Redis Cluster 并没有使⽤⼀致性哈希，采⽤的是 哈希槽分区 ，每⼀个键值对都属于⼀个 hash slot（哈希槽） 。

![image-20240602193639081](https://gitee.com/fushengshi/image/raw/master/image-20240602193639081.png)

客户端连接 Redis Cluster 中任意⼀个 Master 节点即可访问 Redis Cluster 的数据，当客户端发送命令请求的时候，需要先根据 key 通过上⾯的计算公示找到的对应的哈希槽，然后再查询哈希槽和节点的映射关系，即可找到⽬标节点。

- 根据键值对的 key，按照 CRC16 算法计算一个 16 bit 的值。
- 再用 16 bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。

Redis Cluster 扩容和缩容本质是进⾏重新分⽚，动态迁移哈希槽（以哈希槽为单位）。为了保证 Redis Cluster 在扩容和缩容期间依然能够对外正常提供服务，Redis Cluster 提供了重定向机制：

- ASK 重定向

  客户端请求的 key 对应的哈希槽当前正在迁移⾄新的节点，就会返回 ASK 重定向错误，告知客户端要将请求发送到哈希槽被迁移到的⽬标节点。不会同步更新客户端缓存的哈希槽分配信息。

- MOVED 重定向

  客户端请求的 key 对应的哈希槽迁移完成，就会返回 MOVED 重定向错误，告知客户端当前哈希槽是由哪个节点负责，客户端向⽬标节点发送请求并更新缓存的哈希槽分配信息。

Redis选择使用哈希槽的原因在于其灵活性、稳定性、数据分布和算法复杂度等方面的优势。

- 哈希槽可以灵活配置每个节点占用的槽位数量，当节点数量变化时，只需重新分配少量槽位，使得数据迁移更加高效。

  > 一致性哈希虽然能够均匀分配数据，但当节点增减时，需要重新分配大量数据。

- 哈希槽的节点宕机只会影响一小部分槽位，可以通过主从复制加哨兵模式保证高可用性。

  > 一致性哈希的节点宕机可能导致大量槽位不可用，这称为雪崩现象。

- 哈希槽可以手动控制槽位大小，更灵活地控制数据分布。

  > 一致性哈希虽然能保证数据均匀分布，但在某些情况下，数据的分布可能过于集中在某些节点上。

- 哈希槽算法相对简单。

  > 一致性哈希算法相对复杂，实现和维护的难度较大。



#### Gossip 协议

Redis Cluster 中的各个节点基于 Gossip 协议 共享信息，每个 Redis 节点都维护了⼀份集群的状态信息。

Redis Cluster相当于是内置了 Sentinel 机制，Redis Cluster 内部的各个 Redis 节点通过 Gossip 协议互相探测健康状态，在故障时可以⾃动切换。

- MEET

  在 Redis Cluster 中的某个 Redis 节点上执⾏ `CLUSTER MEET ip port` 命令，可以向指定的 Redis 节点发送⼀条 MEET 信息，⽤于将其添加进 Redis Cluster 成为新的 Redis 节点。

- PING/PONG

  Redis Cluster 中的节点都会定时地向其他节点发送 PING 消息，来交换各个节点状态信息，检查各个节点状态，包括在线状态、疑似下线状态 PFAIL 和已下线状态 FAIL。

- FAIL

  Redis Cluster 中的节点 A 发现 B 节点 PFAIL ，并且在下线报告的有效期限内集群中半数以上的节点将 B 节点标记为 PFAIL，节点 A 就会向集群⼴播⼀条 FAIL 消息，通知其他节点将故障节点 B 标记为 FAIL 。



## 分布式缓存

从数据一致性的角度来说，缓存本身也有集群部署的需求。根据分布式缓存集群是否能保证数据一致性可以将它分为 AP 和 CP 两种类型。

> Redis 集群就是典型的 AP 式，它具有高性能、高可用等特点，并不保证强一致性。
>
> 能够保证强一致性的 ZooKeeper、Doozerd、Etcd 等分布式协调框架，通常不会当作缓存框架使用，这些分布式协调框架的吞吐量相对 Redis 来说非常有限。
>
> 不过，ZooKeeper、Doozerd、Etcd 常跟 Redis 和其他分布式缓存搭配工作，用来实现其中的通知、协调、队列、分布式锁等功能。

### 缓存和数据库一致性

#### 缓存模式

- Cache-Aside Pattern（旁路缓存）

  由**应用程序负责**直接从缓存中读取和写入数据。如果缓存未命中，应用程序将从数据库加载数据，并将其存储在缓存中。

  适用于读操作比写操作频繁，对数据一致性要求不是非常严格的场景。

  - 读策略

    应用程序首先查询缓存，如果缓存命中直接返回数据。

    如果缓存数据不存在，应用程序从数据库查询数据，然后将数据存储在缓存中并返回给用户。

  - 写策略

    应用程序更新数据库，删除缓存中数据。

- Read/Write Through（读穿缓存 / 写穿缓存）

  策略的核心原则是用户只与缓存打交道，**由缓存和数据库通信**写入或者读取数据 。

  - Read Through

    首先查询缓存，如果存在则直接返回。

    如果缓存数据不存在，由缓存组件负责从数据库中同步加载数据。

  - Write Through

    首先查询缓存，如果存在则更新缓存中的数据，并且由缓存组件**同步**更新到数据库中。

    如果缓存数据不存在，直接更新数据库。

  > 经常使用的分布式缓存组件 Memcached、Redis 都不提供写入数据库，或者自动加载数据库中的数据的功能。

- Write-Back / Write-Behind Pattern（写回）

  - 读策略

    首先查询缓存，如果存在则直接返回。

    如果缓存数据不存在，则寻找一个可用的缓存块。

    - 如果这个缓存块是脏的，就把缓存块中之前的数据写入到后端存储中，并且从后端存储加载数据到缓存块。
    - 如果不是脏的，则由缓存组件将后端存储中的数据加载到缓存中，并缓存设置为不是脏的，返回数据。

  - 写策略

    应用程序首先将数据写入缓存，并把缓存标记为 脏（dirty）的，然后异步地更新后端数据源。

  > 计算机体系结构中的策略，如 MySQL 中 Buffer Pool 的缓存页、Linux 的 Page Cache、CPU 的 CPU  Cache。



#### 一致性问题

旁路缓存模式（先更新数据库，后删除缓存）不能保证在一致性上绝对不出问题。

- 并发问题

  ![image-20240724015701682](https://gitee.com/fushengshi/image/raw/master/image-20240724015701682.png)

  对于旁路缓存模式，两个线程并发读写数据：

  1. 缓存中 X 不存在。
  2. 线程 A 读取数据库，得到旧值。
  3. 线程 B 更新数据库。
  4. 线程 B 删除缓存。
  5. 线程 A 将旧值写入缓存。

  最终缓存中是旧值，数据库中是新值，缓存不一致。这种情况理论来说是可能发生的，但是**概率很低**，需要同时满足：

  1. 数据是从未被缓存过或缓存刚好失效。

  2. 读请求和写请求并发。

  3. 线程 A 缓存的写入晚于数据库的写更新和删除。

     缓存的写入通常远远快于数据库的写入，一旦线程 A 早于线程 B 清空缓存之前更新了缓存，接下来的请求就会因为缓存为空而从数据库中重新加载数据，不会出现数据不一致。


- 删除缓存失败

  删除缓存失败会导致数据库和缓存不一致，保证删除缓存成功执行是解决问题的关键。

  - 缓存失效时间变短

    让缓存数据的过期时间变短，失效后缓存就会从数据库中加载数据。（治标不治本）

  - 增加缓存更新重试机制

    消息队列：将删除缓存重试的消息投递到消息队列，然后由专门的消费者来重试。

    订阅数据库变更日志：MySQL 举例，当一条数据发生修改时，MySQL 就会产生一条变更日志（Binlog），可以订阅这个日志，拿到具体操作的数据，然后再根据这条数据，去删除对应的缓存。

    > 订阅变更日志有成熟的开源中间件，例如阿里的 Canal。




### 缓存问题

#### 缓存雪崩

指缓存**同一时间大面积的失效**，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。

- 大量数据同时过期

  - 均匀设置过期时间

    缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。

  - 永不过期

    逻辑上永不过期给每一个缓存数据增加相应的缓存标记，缓存标记失效则更新数据缓存。

    或者不给热点数据设置过期时间，由后台异步更新缓存。

  - 互斥锁

    当业务线程在处理用户请求时，如果发现访问的数据不在 Redis 里，就加个互斥锁，保证同一时间内只有一个请求来构建缓存。

- Redis 故障宕机

  - 服务熔断

    暂停业务应用对缓存服务的访问，**直接返回错误**，不用再继续访问数据库，从而降低对数据库的访问压力，保证数据库系统的正常运行，然后等到 Redis 恢复正常后，再允许业务应用访问缓存服务。

  - 请求限流

    只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务，等到 Redis 恢复正常并把缓存预热完后，再解除请求限流的机制。

  - 构建 Redis 缓存高可靠集群

    通过主从节点的方式构建 Redis 缓存高可靠集群。

#### 缓存击穿

如果缓存中的**某个热点数据过期**了，此时大量的请求访问该热点数据无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮。

- 互斥锁

  保证同一时间只有一个业务线程更新缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。

- 永不过期

  不给热点数据设置过期时间，由后台异步更新缓存。

  或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间。

  > 缓存数据不设置有效期，并不是意味着数据一直能在内存里，因为**当系统内存紧张的时候，有些缓存数据会被淘汰**。

#### 缓存穿透

当用户访问的数据，**既不在缓存中，也不在数据库中**，请求在访问缓存时发现缓存缺失，再去访问数据库时发现数据库中也没有要访问的数据，没办法构建缓存数据服务后续的请求。当有大量这样的请求到来时，数据库的压力骤增。

- 缓存空值或者默认值

  线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值（缓存有效时间可以设置短点），这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。

- 布隆过滤器

  使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在。

  缺点：

  - 判断元素是否在集合中时有一定错误几率

    查询布隆过滤器数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，数据库中一定就不存在这个数据。

  - 不支持删除元素

- 非法请求的限制

  在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。



## 分布式锁

分布式系统下，不同的服务 / 客户端通常运行在独立的 JVM 进程上。如果多个 JVM 进程共享同一份资源的话，使用本地锁就没办法实现资源的互斥访问了。于是分布式锁就诞生了。

- 加锁

  Redis `SET` 命令可以实现分布式锁。

  ```shell
  SET lock_key unique_value NX PX 10000
  ```

  - `NX` 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作。
  - `PX` 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。

  如果操作共享资源的时间大于过期时间，就会出现锁提前过期的问题，进而导致分布式锁直接失效。

- 解锁

  解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。

  ```lua
  // 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  ```

### Redisson

Redisson 是一个开源的 Java 语言 Redis 客户端，提供了很多开箱即用的功能，不仅仅包括多种分布式锁的实现。并且，Redisson 还支持 Redis 单机、Redis Sentinel、Redis Cluster 等多种部署架构。

Redisson 中的分布式锁自带**自动续期机制**。提供了一个专门用来监控和续期锁的 Watch Dog（看门狗），如果操作共享资源的线程还未执行完成的话，Watch Dog 会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。

- 在设置锁的过期时间时，尽量根据业务逻辑的执行时间来估算一个合理的值。这个值应该足够长，以覆盖大部分情况下业务逻辑的执行时间。
- 逻辑执行的时间超过看门狗超时时间（如 10 秒），会自动为锁续期 10 秒的过期时间，使锁的过期时间保持在锁过期时间（如 30 秒）。
- 分布式锁的续期是在客户端执行的，所以如果客户端宕机，续期线程就不能续期。

```lua
if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then
    redis.call('pexpire', KEYS[1], ARGV[1]);
    return 1;
end;
return 0;
```



### Redlock

Redis 集群下，上面介绍到的分布式锁的实现会存在一些问题。由于 Redis 集群数据同步到各个节点时是异步的，如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。

Redlock 算法让客户端向 Redis 集群中的多个独立的 Redis 实例依次请求申请加锁，如果客户端能够和半数以上的实例成功地完成加锁操作并且使用的时间小于锁失效时间时，认为客户端成功地获得分布式锁，否则加锁失败。

> Redlock 实现比较复杂，性能比较差，发生时钟变迁的情况下还存在安全性隐患，**实际项目中不建议使用 Redlock 算法**，成本和收益不成正比。
>
> 如果必须要实现一个绝对可靠的分布式锁，可以基于 ZooKeeper ，只是性能会差一些。

1. 客户端 1 从 Redis 节点 A, B, C 成功获取了锁。由于网络问题，无法访问 D 和 E。
2. 节点 C 上的**时钟发生了向前跳跃，导致它上面维护的锁过期了**。
3. 客户端 2 从 Redis 节点 C, D, E 成功获取了同一个资源的锁。由于网络问题，无法访问 A 和 B。
4. 现在，客户端 1 和客户端 2 都认为自己持有了锁。



## 内存管理

###  过期删除

Redis 选择惰性删除 + 定期删除这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。

- 惰性删除

  不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。

- 定期删除

  每隔一段时间随机从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。

### 内存淘汰

在设置了过期时间的数据中进行淘汰：

- volatile-random

  随机淘汰设置了过期时间的任意键值。

- volatile-ttl

  优先淘汰更早过期的键值。

- volatile-lru

  淘汰所有设置了过期时间的键值中，最久未使用的键值。

  Redis 实现的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是 Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳。

  当 Redis 进行内存淘汰时，会使用**随机采样的方式来淘汰数据**，随机取 5 个值（此值可配置），淘汰最久没有使用的那个。

  > Redis 3.0 之前，默认的内存淘汰策略。
  >
  > 无法解决缓存污染问题，应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。

- volatile-lfu

  淘汰所有设置了过期时间的键值中，最少使用的键值。

  Redis 对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt（Last Decrement Time），低 8bit 存储 logc（Logistic Counter）。

  - ldt 是用来记录 key 的访问时间戳。

  - logc 是用来记录 key 的访问频次，它的值越小表示使用频率越低，越容易淘汰。

    logc 并不是单纯的访问次数，而是访问频次（访问频率），因为 **logc 会随时间推移而衰减的**。

  > Redis 4.0 后新增的内存淘汰策略。

在所有数据范围内进行淘汰：

- allkeys-random

  随机淘汰任意键值。

- allkeys-lru

  淘汰整个键值中最久未使用的键值。

- allkeys-lfu

  淘汰整个键值中最少使用的键值。



## 线程模型

![image-20240817174727778](https://gitee.com/fushengshi/image/raw/master/image-20240817174727778.png)

对于读写命令来说，Redis 一直是单线程模型。

> 在 Redis 4.0 版本之后引入了多线程来执行一些大键值对的异步删除操作。
>
> Redis 6.0 版本之后采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上，所以为了提高网络请求处理的并行度，Redis 6.0 对于网络请求采用多线程来处理（对于读写命令，Redis 仍然使用单线程来处理）。

- Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构，比如哈希表和跳表。
- 单线程模型避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。
-  Redis 采用了 I/O 多路复用机制处理大量的客户端 Socket 请求，这让 Redis 可以高效地进行网络通信。

> 对于每个命令的执行时间是有要求的。如果某个命令执行过长，会造成其他命令的阻塞，对于 Redis 这种高性能的服务来说是致命的，所以 Redis 是面向快速执行场景的数据库。

**文件事件处理器**：

Redis 基于 Reactor 模式 开发了文件事件处理器（file event handler）。

- 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。
- 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。

![image-20240707140515733](https://gitee.com/fushengshi/image/raw/master/image-20240707140515733.png)



## 数据类型

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240519155956497.png" alt="image-20240519155956497" style="zoom: 67%;" />

### String

String 类型的底层的数据结构

- int
- SDS（简单动态字符串）

#### 应用场景

- 缓存对象

- 常规计数

  Redis 处理命令是单线程，所以执行命令的过程是原子的。

- 分布式锁

  SET 命令有个 NX 参数可以实现 key 不存在才插入，可以用它来实现分布式锁。

  ```shell
  SET lock_key unique_value NX PX 10000
  ```

  解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以原子性的方式执行，保证了锁释放操作的原子性。

  ```lua
  // 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  ```

- 共享 Session 信息

  在开发后台管理系统时，会使用 Session 来保存用户的会话（登录）状态，这些 Session 信息会被保存在服务器端，只适用于单系统应用，如果是分布式系统此模式将不再适用。

  > 可用 JWT 代替。

### List

List 类型的底层数据结构：

- quicklist

  Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表（数据量比较少时使用）。

#### 应用场景

- 消息队列

### Hash

Hash 类型的底层数据结构：

- 哈希表

- ~~压缩列表~~ listpack

  数据量比较少时使用。

  > 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构实现。

#### 应用场景

- 缓存对象

  Hash 类型的 （key，field， value） 的结构与对象的（对象id， 属性， 值）的结构相似，也可以用来存储对象。

  以用户 id 为 key，商品 id 为 field，商品数量为 value，恰好构成了购物车的3个要素。

### Set

Set 类型的底层数据结构：

- 哈希表
- 整数集合（数据量比较少时使用）

### Zset

Zset 类型的底层数据结构：

- 跳表

- ~~压缩列表~~ listpack

  数据量比较少时使用。

  > 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构实现。

```c
typedef struct zset { // zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。
    dict *dict;
    zskiplist *zsl;
} zset;
```

Zset 类型（有序集合类型）相比于 Set 类型多了一个排序属性 score（分值），对于有序集合 ZSet 来说，每个存储元素相当于有两个值组成的，一个是有序集合的元素值，一个是排序值。

```shell
# 往有序集合key中加入带分值元素
ZADD key score member [[score member]...]
# 为有序集合key中元素member的分值加上increment
ZINCRBY key increment member

# 正序获取有序集合key从start下标到stop下标的元素
ZRANGE key start stop [WITHSCORES]
# 倒序获取有序集合key从start下标到stop下标的元素
ZREVRANGE key start stop [WITHSCORES]

# 返回有序集合中指定分数区间内的成员，分数由低到高排序。
ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]
```

#### 应用场景

- 排行榜

### BitMap

Bitmap 位图，是一串连续的二进制数组（0 和 1），可以通过偏移量（Offset）定位元素。BitMap 通过最小的单位 bit 来进行 `0|1` 的设置，表示某个元素的值或者状态，时间复杂度为 O(1)。

Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。



## 数据结构

### SDS

Redis 是用 C 语言实现的，但是它没有直接使用 C 语言的 `char*` 字符数组来实现字符串，而是自己封装了一个名为简单动态字符串（simple dynamic string，SDS） 的数据结构来表示字符串。

```c
struct __attribute__ ((__packed__)) sdshdr16 {
    uint16_t len; // 字符数组长度和分配空间大小不能超过 2 的 16 次方
    uint16_t alloc; 
    unsigned char flags; 
    char buf[];
};
```

- SDS 不仅可以保存文本数据，还可以保存二进制数据。

  因为 SDS 使用 `len` 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。

- SDS 获取字符串长度的时间复杂度是 O(1)。

  SDS 结构里用 `len` 属性记录了字符串长度，所以复杂度为 O(1)。

- Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。

  SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。

### 双向链表

Redis 的 List 对象的底层实现之一就是链表。

```c
typedef struct listNode {
    // 前置节点
    struct listNode *prev;
    // 后置节点
    struct listNode *next;
    // 节点的值
    void *value;
} listNode;
```

> Redis 3.0 的 List 对象在数据量比较少的情况下，会采用压缩列表作为底层数据结构的实现，它的优势是节省内存空间，并且是内存紧凑型的数据结构。
>
> 压缩列表存在性能问题，所以 Redis 3.2 版本设计了新的数据结构 quicklist，并将 List 对象的底层数据结构改由 quicklist 实现。

### 压缩列表

压缩列表是 是**由连续内存块组成的顺序型数据结构**，有点类似于数组。

一种内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，这种方法可以有效地节省内存开销。

![image-20240519161036376](https://gitee.com/fushengshi/image/raw/master/image-20240519161036376.png)

查找元素时只能逐个查找，复杂度 O(N) ，因此压缩列表不适合保存过多的元素。

压缩列表的 entry 保存 prevlen 是为了实现节点从后往前遍历，知道前一个节点的长度，就可以计算前一个节点的偏移量。

连锁更新：

压缩列表新增某个元素或修改某个元素时，如果空间不不够，压缩列表占用的内存空间就需要重新分配。而当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起**连锁更新**问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降。

### listpack

quicklistNode 还是用了压缩列表来保存元素，压缩列表连锁更新的问题，来源于它的结构设计。

![image-20240519162109679](https://gitee.com/fushengshi/image/raw/master/image-20240519162109679.png)

listpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题。

### quicklist

quicklist 就是双向链表 + 压缩列表组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表。

> 在 Redis 3.0 之前，List 对象的底层数据结构是双向链表或者压缩列表。然后在 Redis 3.2 的时候，List 对象的底层改由 quicklist 数据结构实现。

```c
typedef struct quicklistNode {
    // 前一个quicklistNode
    struct quicklistNode *prev;
    // 下一个quicklistNode
    struct quicklistNode *next;
    // quicklistNode指向的压缩列表
    unsigned char *zl;              
    // 压缩列表的的字节大小
    unsigned int sz;                
    // 压缩列表的元素个数
    unsigned int count : 16;
    ....
} quicklistNode;
```

向 quicklist 添加一个元素的时候，不会像普通的链表那样新建一个链表节点，而是会检查插入位置的压缩列表是否能容纳该元素：

- 如果能容纳就直接保存到 quicklistNode 结构里的压缩列表。
- 如果不能容纳，新建一个新的 quicklistNode 结构。

quicklist 会控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题。

### 哈希表

哈希表是一个数组（`dictEntry **table`），数组的每个元素是一个指向哈希表节点（dictEntry）的指针。Redis 采用了链式哈希来解决哈希冲突，被分配到同一个哈希桶上的多个节点可以用这个单项链表连接起来。

```c
typedef struct dictht {
    // 哈希表数组
    dictEntry **table;
    // 哈希表大小
    unsigned long size;  
    // 哈希表大小掩码，用于计算索引值
    unsigned long sizemask;
    // 该哈希表已有的节点数量
    unsigned long used;
} dictht;

typedef struct dictEntry {
    // 键值对中的键
    void *key;
  
    // 键值对中的值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    // 指向下一个哈希表节点，形成链表
    struct dictEntry *next;
} dictEntry;
```

#### 渐进式 rehash

Redis 采用了渐进式 rehash 避免 rehash 在数据迁移过程中因拷贝数据的耗时影响 Redis 性能的情况。将一次性大量数据迁移工作的开销，分摊到了多次处理请求的过程中，避免了一次性 rehash 的耗时操作。

每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将哈希表 1 中索引位置上的所有 key-value 迁移到哈希表 2 上。

在进行渐进式 rehash 的过程中会有两个哈希表，在渐进式 rehash 进行期间哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行。新增一个 key-value 时，会被保存到 哈希表 2 里面，而 哈希表 1 则不再进行任何添加操作。

### 跳表

跳表是在链表基础上改进过来的，实现了一种多层的有序链表，能快读定位数据。

![image-20240519190519218](https://gitee.com/fushengshi/image/raw/master/image-20240519190519218.png)



```c
typedef struct zskiplistNode {
    // Zset对象的元素值
    sds ele;
    // 元素权重值
    double score;
    // 后向指针
    struct zskiplistNode *backward;
  
    // 节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;
```

跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。具体的做法是，跳表在创建节点时候，会生成范围为 [0-1] 的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。

为什么用跳表而不用平衡树：

- 内存占用

  平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。Redis 里实现取 p=1/4，平均每个节点包含 1.33 个指针，比平衡树更有优势。

- 范围查找

  Zset 经常需要执行 ZRANGE 或 ZREVRANGE 的命令，即作为链表遍历跳表，跳表的缓存局部性与其他类型的平衡树一样好。

  跳表比平衡树操作简单：

  - 平衡树找到指定范围的小值之后，需要中序遍历寻找不超过大值的节点。
  - 跳表上只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可实现。

- 算法实现难度

  跳表比平衡树要简单得多：

  - 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂。
  - 跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。

#### 跳表查询

查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：

- 如果当前节点的权重小于要查找的权重时，跳表就会访问该层上的下一个节点。
- 如果当前节点的权重等于要查找的权重时，并且当前节点的 SDS 类型数据小于要查找的数据时，跳表就会访问该层上的下一个节点。

- 如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，相当于跳到下一层接着查找。

示例，如果要查找 [元素：abcd，权重：4] 的节点：

- 先从头节点的最高层开始，L2 指向了 [元素：abc，权重：3] 节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点。
- 但是该层的下一个节点是空节点（leve[2] 指向的是空节点），于是就会跳到 [元素：abc，权重：3] 节点的下一层去找，也就是 leve[1]。
- [元素：abc，权重：3] 节点的 leve[1] 的下一个指针指向了 [元素：abcde，权重：4] 的节点，然后将其和要查找的节点比较。虽然[元素：abcde，权重：4] 的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据大于要查找的数据，所以会继续跳到 [元素：abc，权重：3] 节点的下一层去找，也就是 leve[0]。
- [元素：abc，权重：3] 节点的 leve[0] 的下一个指针指向了 [元素：abcd，权重：4] 的节点，该节点正是要查找的节点，查询结束。



# 附录

## 分布式锁

### 基于 Redis 的分布式锁

见上。

### 基于 Zookeeper 的分布式锁

Redis 实现分布式锁性能较高，ZooKeeper 实现分布式锁可靠性更高。实际项目中应该根据业务的具体需求来选择。

ZooKeeper 分布式锁是基于 **临时顺序节点** 和 **Watcher（事件监听器）** 实现的。

- 加锁
  1. 首先我们要有一个持久节点 `/locks`，客户端获取锁就是在 `locks` 下创建临时顺序节点。
  2. 假设客户端 1 创建了 `/locks/lock1` 节点，创建成功之后，会判断  `lock1` 是否是 `/locks` 下最小的子节点。
  3. 如果 `lock1` 是最小的子节点，则获取锁成功。否则，获取锁失败。
  4. 如果获取锁失败，则说明有其他的客户端已经成功获取锁。客户端 1 并不会不停地循环去尝试加锁，而是在前一个节点比如 `/locks/lock0` 上注册一个事件监听器。这个监听器的作用是当前一个节点释放锁之后通知客户端 1（避免无效自旋），这样客户端 1 就加锁成功了。

- 释放锁
  1. 成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。
  2. 成功获取锁的客户端在出现故障之后，对应的子节点由于是临时顺序节点，也会被自动删除，避免了锁无法被释放。
  3. 事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放。

> Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。



## NoSQL

NoSQL 指的是不同于传统的关系型数据库的其他数据库系统的统称。

-  KV 存储

这类存储相比于传统的数据库的优势是极高的读写性能，一般对性能有比较高的要求的场景会使用。

> Redis、LevelDB

- 列式存储数据库

  这种数据库的特点是数据不像传统数据库以行为单位来存储，而是以列来存储，适用于一些离线数据统计的场景。

  > Hbase、Cassandra

- 文档型数据库

  这种数据库的特点是 Schema Free（模式自由），数据表中的字段可以任意扩展，比如说电商系统中的商品有非常多的字段，并且不同品类的商品的字段也都不尽相同，使用关系型数据库就需要不断增加字段支持，而用文档型数据库就简单很多了。

  > MongoDB、CouchDB



## 环形缓冲

环形缓冲是一种拥有读、写两个指针的数据复用结构，在计算机科学中有非常广泛的应用。

以 Caffeine 为代表的异步日志提交机制参考了经典的数据库设计理论，它把对数据的读、写过程看作是日志（即对数据的操作指令）的提交过程。设有专门的环形缓存区（Ring Buffer，Circular Buffer）记录由于数据读取而产生的状态变动日志。为了进一步减少数据竞争，Caffeine 给每条线程（对线程取 Hash，哈希值相同的使用同一个缓冲区）都设置了一个专用的环形缓冲。

![f23ca15743d34c4b9c9ea272ec506ca7](https://gitee.com/fushengshi/image/raw/master/f23ca15743d34c4b9c9ea272ec506ca7.jpg)




