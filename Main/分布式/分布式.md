# 分布式

## CAP 理论

- 一致性（Consistency）

  所有节点访问同一份最新的数据副本。

- 可用性（Availability）

  非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。

- 分区容错性（Partition Tolerance）

  分布式系统出现网络分区的时候，仍然能够对外提供服务。

分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。



## BASE 理论

- 基本可用（Basically Available）

  基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。不等价于系统不可用。

- 软状态（Soft-state）

  软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

- 最终一致性（Eventually Consistent）

  最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。



## 分布式共识算法

共识算法的作用是让分布式系统中的多个节点之间对某个提案（Proposal）达成一致的看法。提案的含义在分布式系统中十分宽泛，像哪一个节点是 Leader 节点、多个事件发生的顺序等等都可以是一个提案。

### Bully 算法

Bully 算法选举原则是长者为大，即在所有活着的节点中，选取ID最大的节点为主节点。

- 当主节点发生故障或其他原因导致重新选主时，如果当前节点发现自己的ID不是当前活着的节点中ID最大的，则向比自己ID大的所有节点发送 Election 消息，并等待回复 Alive 消息。
- 在给定的时间范围内，本节点如果没有收到其他节点回复的 Alive 消息，则认为自己成为Leader，并且向其他节点发送 Victory 消息。  如果接受到比自己 ID 大的节点的 Alive 消息则等待 Victory 消息。



### Paxos 算法

Paxos 算法是第一个被证明完备的分布式系统共识算法，描述的是多节点之间如何就某个值（提案 Value）达成共识。

#### Basic Paxos

首先有两种角色，一个是提议者，一个是接受者。提议者可以向接受者提出提议，然后接受者表达意见。因为存在多个提议者，如果同时表达意见会出现意见不一致的情况，所以首先需要尽快选出一个领导者，让意见统一。然后领导者会给接受者发出提议，如果一个提议被大多数接受者接纳，这个提议就通过了。

![image-20240707225206051](https://gitee.com/fushengshi/image/raw/master/image-20240707225206051.png)

- 第一阶段 准备（Prepare）

  > 具体方法就是通过编号，提议者会先报告一个编号，谁的编号最大谁就是领导（贿选）。

  第一阶段就相当于抢占锁的过程。如果某个提案节点准备发起提案，必须先向所有的决策节点广播一个许可申请（称为 Prepare 请求）。提案节点的 Prepare 请求中会附带一个全局唯一的数字 n 作为提案 ID，决策节点收到后，会给提案节点两个承诺和一个应答。

  - 两个承诺

    承诺不会再接受提案 ID 小于或等于 n 的 Prepare 请求，承诺不会再接受提案 ID 小于 n 的 Accept 请求。

  - 一个应答

    在不违背以前作出的承诺的前提下，回复已经批准过的提案中ID最大的那个提案所设定的值和提案ID，如果该值从来没有被任何提案设定过，则返回空值。

- 第二阶段 批准（Accept）

  当提案节点收到了多数派决策节点的应答（称为 Promise 应答）后，可以开始第二阶段过程。上一轮胜出的领导者提出提议，发送给各个接受者。

  - 如果提案节点发现所有响应的决策节点此前都没有批准过这个值（即为空），就说明它是第一个设置值的节点，可以随意地决定要设定的值。并将自己选定的值与提案 ID，构成一个二元组（id, value），再次广播给全部的决策节点（Accept 请求）。
  - 如果提案节点发现响应的决策节点中，已经有至少一个节点的应答中包含有值了，那它就不能够随意取值了，必须无条件地从应答中找出提案 ID 最大的那个值并接受，构成一个二元组（id, maxAcceptValue），然后再次广播给全部的决策节点（Accept 请求）。

  > 只要 Leader 是相对稳定不变的，第一阶段就不必要。系统可以在接下来的 Paxos 算法实例中，跳过的第一阶段，直接使用同样的 Leader。

#### Multi-Paxos

Multi Paxos 对 Basic Paxos 的核心改进是增加了**选主**的过程：

- 提案节点会通过定时轮询（心跳），确定当前网络中的所有节点里是否存在一个主提案节点。
- 一旦没有发现主节点存在，节点就会在心跳超时后使用 Basic Paxos 中定义的准备、批准的两轮网络交互过程，向所有其他节点广播自己希望竞选主节点的请求，希望整个分布式系统对 选主 协商达成一致共识。
- 如果得到了决策节点中多数派的批准，便宣告竞选成功。

当选主完成之后，除非主节点失联会发起重新竞选，否则就只有主节点本身才能够提出提案。

> Raft 是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。除了 Raft 算法之外，当前最常用的一些共识算法比如 ZAB 协议、 Fast Paxos 算法都是基于 Paxos 算法改进的。



### Raft 协议

每个副本都会处于三种状态之一：

- Leader：所有请求的处理者，Leader 副本接受 client 的更新请求，本地处理后再同步至多个其他副本。

- Follower：请求的被动更新者，从 Leader 接受更新请求，然后写入本地日志文件。

- Candidate：如果 Follower 副本在一段时间内没有收到 Leader 副本的心跳，则判断 Leader 可能已经故障，启动选主过程，此时副本会变成 Candidate 状态，直到选主结束。

日志：

- entry：每一个事件成为 entry，只有 Leader 可以创建 entry。

  entry 的内容为 `<term,index,cmd>`，其中 cmd 是可以应用到状态机的操作。

- log：由 entry 构成的数组，每一个 entry 都有一个表明自己在 log 中的 index，只有 Leader 才可以改变其他节点的 log。

  entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。

  Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中。

> 和 Paxos 的基本理念一样，首先最基本的 Flollow（接受者），如果 Follower 接收不到 Leader 的心跳，就会转化为Candidate（提议者），然后提议者开始**贿选**，直到选出 Leader 之后，所有没选上的 Candidate 退回到 Follower 状态，统一接收 Leader 领导。

#### Leader 选举

在 Raft 协议中，将时间分成了一些任意长度的**时间片 term**，term 使用连续递增的编号的进行识别。每一个任期的开始都是一次选举，在选举开始时，一个或多个 Candidate 会尝试成为 Leader。如果一个 Candidate 赢得了选举，它就会在该任期内担任 Leader。如果没有选出 Leader，将会开启另一个任期，并立刻开始下一次选举。

![image-20240610035823486](https://gitee.com/fushengshi/image/raw/master/image-20240610035823486.png)

每个 Follower 会有一个倒计时（election timeout），在倒计时结束之前，如果没有收到任何 Leader 的心跳，或者其他 Candidate 的投票请求，就会转化为 Candidate，开始选举。

变成 Candidate 之后，会先投自己一票，同时开启一个倒计时，然后向所有其他节点发起投票请求。

当 Candidate 状态时收到来自 Leader 的心跳消息，比较自己的 term 编号和 Leader 的 term 编号：

- 该 Leader 的 term 号大于等于自己的 term 号，说明对方已经成为 Leader，则自己回退为 Follower。
- 该 Leader 的 term 号小于自己的 term 号，那么会拒绝该请求并让该节点更新 term。

Follower如果收到了不如自己新（entry 的 index）的 Candidate 的投票请求就会将其丢弃。

> term 大的优先，然后 entry 的 index 大的优先。

如果发出去的投票请求在一个 term 内得到了超过半数节点的成功回应，就会成为 Leader，并周期性地向其它节点广播心跳消息，心跳会重置每个节点的 election timeout 倒计时时间。

如果在倒计时完成之前，没有成为 Leader 或者接收到其他 Leader 的消息，就会发起新一轮选举。

#### 主从同步

- 当一个数据修改的请求过来，会直接找到 Leader 节点，每一条命令都会被按顺序记录到Leader的日志中，每一条命令都包含term编号和顺序索引，然后同步给各个 Follower。

- **大多数节点成功复制后**，Leader 就会提交命令，即执行该命令并且将执行结果返回客户端。

  > Quorum 机制：一旦系统中超过半数的节点完成了状态的转换，就可以认为数据的变化已经被正确地存储在了系统当中。这样就可以容忍少数（通常是不超过半数）的节点失联，使得增加机器数量可以用来提升系统整体的可用性。

#### 崩溃恢复

![image-20240610032848277](https://gitee.com/fushengshi/image/raw/master/image-20240610032848277.png)

- 在 Raft 中，Leader 通过强制 Follower 复制自己的日志来解决日志不一致，冲突的日志将会被重写。先找到最新的一致的那条日志，然后把 Follower 之后的日志全部删除，Leader 再把自己在那之后的日志推送给 Follower。

  投票过程确保只有拥有全部已提交日志的 Candidate 能成为 Leader，Follower 如果收到了不如自己新（entry 的 index）的 Candidate 的投票请求就会将其丢弃。

- 如果命令已经被复制到了大部分节点上，但是还没有提交就崩溃了，Raft 通过让 Leader 统计当前 term 内还未提交的命令已经被复制的数量是否半数以上，然后进行提交。




### ZAB 协议

Zookeeper 基于 ZAB（Zookeeper Atomic Broadcast），实现了主备模式下的系统架构，保持集群中各个副本之间的数据一致性。

ZAB 协议定义了选举（election）、发现（discovery）、同步（sync）、广播（Broadcast）四个阶段。

- 选举（election）：选主。

- 发现（discovery）、同步（sync）：当主选出后，要做的恢复数据的阶段。

- 广播（Broadcast）：当主机和从选出并同步好数据后，正常的主写同步从写数据的阶段。

集群副本有三种状态：

- Leader：领导者。
- Follower：接受提议的跟随者。
- Observer：可以认为是领导者的的 Copy，不参与投票。

#### Leader 选举

每次写成功的消息，都有一个全局唯一的标识 zxid。是 64 bit 的正整数，高 32 为叫 epoch 表示选举纪元，低 32 位是自增的 id，每写一次加一。全局唯一的 zxid 能够给选举和同步数据区分出优先级，核心思想是**递增的 zxid 顺序，保证了能够有优先级最高的节点当主。**

默认基于 TCP 的 FastLeaderElection：

Leader 是检测是否过半 Follower 心跳回复了，Follower 检测 Leader 是否发送心跳了。一旦 Leader 检测失败，则 Leader 进入 Looking 状态，其他 Follower 过一段时间因收不到 Leader 心跳也会进入 Looking 状态，从而出发新的 Leader 选举。

> Raft：只是 Follower 在检测。如过 Follower 在倒计时时间内未收到 Leader 的心跳信息，则 Follower 转变成 Candidate，自增 term 发起新一轮的投票。
>
> ZooKeeper：Leader 和 Follower 都有各自的检测超时方式，Leader 是检测是否过半 Follower 心跳回复了，Follower 检测 Leader 是否发送心跳了。一旦 Leader 检测失败，则 Leader 进入 Looking 状态，其他 Follower 过一段时间因收不到 Leader 心跳也会进入 Looking 状态，从而出发新的 Leader 选举。

每个进入 Looking 状态的节点，会先把投票箱清空，然后通过广播投票给自己，再把投票消息发给其它机器，同时也在接受其他节点的投票。**投票信息包括 选举轮数（electionEpoch）、被投票节点的 zxid，被投票节点的编号**等等。

> Raft 定义了 term 来表示选举轮次。
>
> ZooKeeper 定义了 electionEpoch 来表示选举轮次。

其他 Looking 状态的节点收到后首先判断票是否有效：

- 如果比本地选举轮数小，丢弃。

- 如果比本地选举轮数大，证明自己投票过期了。

  清空本地投票信息，更新选举轮数和结果为收到的内容，通知其他所有节点新的投票方案。

- 如果和本地选举轮数相等，比较优先级（zxid）：

  - 如果收到的优先级小，则忽略该投票。

  - 如果收到的优先级大，则更新自己的投票为对方发过来投票方案，把投票发出去。
  - 如果收到的优先级相等，则更新对应节点的投票。

> Raft：term 大的优先，然后 entry 的 index 大的优先。
>
> ZooKeeper：electionEpoch大的优先，然后 zxid 大的优先。

每收集到一个投票后，查看已经收到的投票结果记录列表，看是否有节点能够达到一半以上的投票数：

- 如果有达到，则终止投票，宣布选举结束，更新自身状态，然后进行发现和同步阶段。
- 如果没有达到，则继续收集投票。

投票终止后，服务器开始更新自身状态，若过半的票投给了自己，则将自己的服务器状态更新为 Leading，否则将自己的状态更新为 Following。

> Raft 中的每个节点在某个 term 轮次内只能投一次票，可能造成分区，即各个 Candidate 都没有收到过半的投票。Raft 通过 Candidate 设置不同的超时时间，来快速解决这个问题，使得先超时 Candidate优先请求来获得过半投票。
>
> ZooKeeper 中的每个节点，在某个 electionEpoch 轮次内，可以投多次票，**只要遇到更优先的就更新，然后分发新的投票给所有节点**。这种情况下不存在分区现象，同时有利于选出含有更新更多的日志的 Server，但是选举时间理论上相对 Raft 要花费的多。

#### 主从同步

见Zookeeper.md。

#### 崩溃恢复

见Zookeeper.md。



### 实际应用

- MySQL 主从使用 Raft 协议。
- Redis 哨兵选主使用 Raft 协议。
- ElasticSearch 集群 Master 选举基于 Bully 和 Paxos 两种算法实现，ES 7.x 加入了基于 Raft 的优化。
- Kafka Kraft 模式下控制器 Leader 选举依赖于内置的 Raft 协议变种（KRaft）实现。
- kv 存储系统 etcd 使用 Raft 协议。
- TiDB 使用 Raft 协议。
- 微信高可用存储系统 PaxosStore 是基于 Multi Paxos 开发的。
- 阿里高性能存储 PolarFS 使用 Raft 变种 ParallelRaft 协议。
- ZooKeeper 使用 ZAB（ZooKeeper Atomic Broadcast）协议。



## 分布式事务

微服务架构下，⼀个系统被拆分为多个⼩的微服务。每个微服务都可能存在不同的机器上，并且每个微服务可能都有⼀个单独的数据库供⾃⼰使⽤。这种情况下，⼀组操作可能会涉及到多个微服务以及多个数据库。分布式事务的终极⽬标就是保证系统中多个相关联的数据库中的数据的⼀致性。

### 两阶段提交 2PC

两阶段提交（2 Phase Commit，2PC）是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成分布式事务的处理。

- 第一阶段

  当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 `prepare` 请求（其中包括事务内容）告诉参与者执行事务，如果能执行就先执行但不提交，执行后回复。

  然后参与者收到 `prepare` 消息后，开始执行事务（但不提交），并将 `Undo log` 和 `Redo log` 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。

- 第二阶段

  第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。

问题：

- 单点故障问题

  一旦协调者宕机，整个系统都处于不可用的状态。

- 性能问题

  参与者收到之后会进行事务的处理但并不提交，整个过程将持续到参与者集群中最慢的那一个处理操作结束为止，影响性能。

  如果此时协调者宕机，参与者占用着的资源都不会再释放发生阻塞。

- 数据不一致问题

  当网络稳定性和宕机恢复能力的假设不成立时，两段式提交可能会出现一致性问题。

  比如当第二阶段，协调者只发送了一部分的 `commit` 请求网络忽然断开，收到消息的参与者会进行事务的提交，没收到的既未提交也没办法回滚，产生数据不一致性。

### 三阶段提交 3PC

与两阶段提交相比，三阶段提交有两个改动点：

- 引入超时机制

  在协调者和参与者中都引入超时机制。

- 插入一个准备阶段

  新增的 CanCommit 是一个询问阶段，协调者让每个参与的数据库根据自身状态，评估该事务是否有可能顺利完成。增加一轮询问阶段，如果都得到了正面的响应，事务能够成功提交的把握就比较大，也意味着因某个参与者提交时发生崩溃而导致全部回滚的风险相对变小。

三个阶段：

- CanCommit 阶段

  协调者向所有参与者发送 `CanCommit` 请求，参与者收到请求后会**查看自身是否能执行事务**，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。

  > 尝试获取数据库锁。
  >
  > 在本地执行事务操作，并将执行结果存储到本地事务日志中，并对该条事务日志进行锁定处理。

- PreCommit 阶段

  协调者根据参与者返回的响应来决定是否可以进行下面的 `PreCommit` 操作。

  - 如果第一阶段参与者返回的都是 YES，那么协调者将向所有参与者发送 `PreCommit` 预提交请求，参与者收到预提交请求后，会进行事务的执行操作，并将 `Undo log` 和 `Redo log` 信息写入事务日志中 ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。

  - 如果在第一阶段协调者收到了任何一个 NO 的信息，或者在一定时间内没有收到全部的参与者的响应（超时），那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，也会中断事务。

  > 本地正式提交该条事务操作，并在完成事务操作后关闭该条会话处理线程、释放系统资源。

- DoCommit 阶段

  这个阶段和 2PC 的第二阶段类似。

  - 如果协调者收到了所有参与者在 `PreCommit` 阶段的 YES 响应，那么协调者将会给所有参与者发送 `DoCommit` 请求，参与者收到 `DoCommit` 请求后则会进行事务的提交工作，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。

  - 若协调者在 `PreCommit` 阶段 收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应（超时），那么就会进行中断请求的发送，参与者收到中断请求后则会通过上面记录的回滚日志进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。

问题：

- 数据不一致问题

  在 `DoCommit` 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，会出现数据不一致性问题。

  > 三段式提交对单点问题和回滚时的性能问题有所改善，但是对一致性风险问题并未有任何改进，甚至是增加了面临的一致性风险。
  >
  > 进入 PreCommit 阶段之后，协调者发出的指令不是 Ack 而是 Abort，而此时因为网络问题，有部分参与者直至超时都没能收到协调者的 Abort 指令的话，这些参与者将会错误地提交事务，这就产生了不同参与者之间数据不一致的问题。

相比 2PC，3PC 主要解决的单点故障问题，并减少阻塞，一旦参与者无法及时收到来自协调者的信息之后会默认执行 commit，而不会一直持有事务资源并处于阻塞状态。

### TCC

TCC 模式不需要依赖于底层数据资源的事务⽀持，需要⼿动实现更多的代码，属于**侵⼊业务代码**的⼀种分布式解决⽅案。

> 可以基于某些分布式事务中间件（如阿里开源的 Seata）完成，以尽量减轻编码工作量。

- Try 阶段

  尝试执⾏，完成业务检查，并预留好必需的业务资源。

- Confirm 阶段

  确认执⾏。当所有事务参与者的 Try 阶段执⾏成功就会执⾏ Confirm，Confirm 阶段会处理 Try 阶段预留的业务资源。否则会执⾏ Cancel。

- Cancel 阶段

  取消执⾏，释放 Try 阶段预留的业务资源。

### Saga

Saga 属于⻓事务解决⽅案，其核⼼思想是将⻓事务拆分为多个本地短事务（本地短事务序列）。

> 可以基于某些分布式事务中间件（如阿里开源的 Seata）完成，以尽量减轻编码工作量。

和 TCC 类似，Saga 正向操作与补偿操作都需要业务开发者⾃⼰实现，因此也属于**侵⼊业务代码**的⼀种分布式解决⽅案。和 TCC 很⼤的⼀点不同是 Saga 没有 Try 动作，它的本地事务 Ti 直接被提交。

Saga 必须保证所有子事务都能够提交或者补偿，但 Saga 系统本身也有可能会崩溃，所以它必须设计成与数据库类似的日志机制（被称为Saga Log），以保证系统恢复后可以追踪到子事务的执行情况



### MQ事务

RocketMQ、Kafka、Pulsar、QMQ 都提供了事务相关的功能。事务允许事件流应⽤将消费、处理、⽣产消息整个过程定义为⼀个原⼦操作。

#### RocketMQ 事务

在 RocketMQ 中使用的是 **事务消息** 和 **事务反查机制** 来解决分布式事务问题。

> RocketMQ 的事务消息也可以被认为是一个两阶段提交。

![image-20240610222218344](https://gitee.com/fushengshi/image/raw/master/image-20240610222218344.png)

- MQ 发送⽅在消息队列上开启⼀个事务，然后发送⼀个**半消息**给 MQ Server/Broker。

  事务提交之前，半消息对于 MQ 订阅⽅/消费者（⽐如第三⽅通知服务）不可⻅。

- 半消息发送成功的话，MQ 发送⽅就开始执⾏本地事务。

- MQ 发送⽅的本地事务执⾏成功的话，半消息变成正常消息，可以正常被消费。

  MQ 发送⽅的本地事务执⾏失败的话，会直接回滚。



### 本地消息表

- 消息生产方额外建一个消息表，记录消息发送状态。

  消息表和业务数据要在一个事务里提交，也就是要在一个数据库里面。然后消息会经过 MQ 发送到消息的消费方。如果消息发送失败，会进行重试发送。

- 消息消费方需要处理这个消息，并完成自己的业务逻辑。

  - 如果本地事务处理成功，表明已经处理成功。

    队列双向确认：消费者消费成功后，也会向消息队列发送一个通知消息。当生产者接收到这条通知消息后，把本地持久化的这条消息的状态设置为完成。

  - 如果处理失败就会执行重试。

    如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。

- 生产方和消费方定时扫描本地消息表，把未处理完成的消息或者失败的消息再发送一遍。



### 尽最大努力通知

利用 MQ 的 ack 机制由 MQ 向消费者发送通知。

-  生产者使用普通消息机制将通知发给 MQ。

- 消费者监听 MQ 接收消息，业务处理完成回应 ack。

- 消费者若没有回应 ack，MQ 会重复通知。

  MQ会按照间隔1min、5min、10min、30min、1h ... 的方式，逐步拉大通知间隔（如果 MQ 采用rocketMq，可在 Broker 中进行配置），直到达到通知要求的时间窗口上限。

- 消费者通过消息校对接口校对消息的一致性。



## 分布式锁

### 基于 Redis 的分布式锁

分布式系统下，不同的服务/客户端通常运行在独立的 JVM 进程上。如果多个 JVM 进程共享同一份资源的话，使用本地锁就没办法实现资源的互斥访问了。于是分布式锁就诞生了。

- 加锁

  Redis `SET` 命令可以实现分布式锁。

  ```shell
  SET lock_key unique_value NX PX 10000
  ```

  - `NX` 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作。
  - `PX` 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。

  如果操作共享资源的时间大于过期时间，就会出现锁提前过期的问题，进而导致分布式锁直接失效。如果操作共享资源的操作还未完成，锁过期**自动续期**。

- 解锁

  解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。

  ```lua
  // 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  ```

#### Redisson

Redisson 是一个开源的 Java 语言 Redis 客户端，提供了很多开箱即用的功能，不仅仅包括多种分布式锁的实现。并且，Redisson 还支持 Redis 单机、Redis Sentinel、Redis Cluster 等多种部署架构。

Redisson 中的分布式锁自带**自动续期机制**。提供了一个专门用来监控和续期锁的 Watch Dog（看门狗），如果操作共享资源的线程还未执行完成的话，Watch Dog 会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。

```lua
if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then
    redis.call('pexpire', KEYS[1], ARGV[1]);
    return 1;
end;
return 0;
```

#### Redlock

Redis 集群下，由于 Redis 集群数据同步到各个节点时是异步的，如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。

[Redlock 算法](https://redis.io/topics/distlock) 让客户端向 Redis 集群中的多个独立的 Redis 实例依次请求申请加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，并且使用的时间小于锁失效时间时，则认为客户端成功地获得分布式锁，否则加锁失败。

Redlock 实现比较复杂，性能比较差，发生时钟变迁的情况下还存在安全性隐患，实际项目中不建议使用 Redlock 算法，成本和收益不成正比。如果必须要实现一个绝对可靠的分布式锁的话，可以基于 ZooKeeper ，只是性能会差一些。

1. 客户端 1 从 Redis 节点 A, B, C 成功获取了锁。由于网络问题，无法访问 D 和 E。
2. 节点 C 上的**时钟发生了向前跳跃，导致它上面维护的锁过期了**。
3. 客户端 2 从 Redis 节点 C, D, E 成功获取了同一个资源的锁。由于网络问题，无法访问 A 和 B。
4. 现在，客户端 1 和客户端 2 都认为自己持有了锁。

### 基于 Zookeeper 的分布式锁

Redis 实现分布式锁性能较高，ZooKeeper 实现分布式锁可靠性更高，实际项目中应该根据业务的具体需求来选择。

ZooKeeper 分布式锁是基于 **临时顺序节点** 和 **Watcher（事件监听器）** 实现的。

- 加锁
  1. 首先我们要有一个持久节点 `/locks`，客户端获取锁就是在 `locks` 下创建临时顺序节点。
  2. 假设客户端 1 创建了 `/locks/lock1` 节点，创建成功之后，会判断 `lock1` 是否是 `/locks` 下最小的子节点。
  3. 如果 `lock1` 是最小的子节点，则获取锁成功。否则，获取锁失败。
  4. 如果获取锁失败，则说明有其他的客户端已经成功获取锁。客户端 1 并不会不停地循环去尝试加锁，而是在前一个节点比如 `/locks/lock0` 上注册一个事件监听器。这个监听器的作用是当前一个节点释放锁之后通知客户端 1（避免无效自旋），这样客户端 1 就加锁成功了。

- 释放锁
  1. 成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。
  2. 成功获取锁的客户端在出现故障之后，对应的子节点由于是临时顺序节点，也会被自动删除，避免了锁无法被释放。
  3. 事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放。

### 基于关系型数据库的分布式锁

- 悲观锁方式

  先查询数据库是否存在记录，为了防止幻读取，通过数据库行锁 `select for update` 锁住这行数据，然后将查询和插入的 SQL 在同一个事务中提交。

  在数据库层面，`select for update` 是悲观锁，会一直阻塞直到事务提交。

- 乐观锁方式

  利用 CAS 机制，并不会对数据加锁，通过对比数据的时间戳或者版本号，实现版本判断。

  比如基于版本号的方式，首先在数据库增加一个 int 型字段 version，然后在 SELECT 同时获取 version值，最后在 UPDATE 的时候检查 version值是否为与得到的版本值相同。

  ```sql
  ## SELECT同时获取version值
  select old_version from Xxx where id = xxx
  
  ## UPDATE 的时候检查version值是否与获取到的值相同
  update Xxx set version = old_version + 1 where id = xxx and version = old_version
  ```



## 分布式 ID

### UUID

UUID 是 Universally Unique Identifier（通用唯一标识符） 的缩写。

```java
UUID.randomUUID()
```

- 数据库主键要尽量短，UUID 的消耗的存储空间比较大（32 个字符串，128 位）。
- UUID 是无顺序的，InnoDB 引擎下，数据库主键的无序性会严重影响数据库性能。

### 雪花算法

Snowflake 由 64 bit 的二进制数字组成：

- sign（1bit）

  符号位（标识正负），始终为 0，代表生成的 ID 为正数。

- timestamp（41 bits）

  一共 41 位，用来表示时间戳，单位是毫秒，可以支撑 2 ^41 毫秒（约 69 年）。

- datacenter id + worker id（10 bits）

  一般来说，前 5 位表示机房 ID，后 5 位表示机器 ID（实际项目中可以根据实际情况调整），可以区分不同集群/机房的节点。

- sequence（12 bits）

  一共 12 位，用来表示序列号。 序列号为自增值，代表单台机器每毫秒能够产生的最大 ID 数（2^12 = 4096），单台机器每毫秒最多可以生成 4096 个 唯一 ID。



### Leaf

Leaf（美团）提供了 **号段模式** 和 **Snowflake** 这两种模式来生成分布式 ID。

利用 proxy server 批量获取，每次获取一个 segment（step决定大小）号段的值。用完之后再去数据库获取新的号段，大大减轻数据库的压力。

采用双buffer的方式，Leaf 服务内部有两个号段缓存区 segment。当前号段已下发 10% 时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前 segment 接着下发，循环往复。

#### 高可用

采用一主两从的方式，同时分机房部署，Master 和 Slave 之间采用**半同步方式**同步数据，这种方案在一些情况会退化成异步模式，甚至在非常极端情况下仍然会造成数据不一致的情况。如果系统要保证 100% 的数据强一致，可以选择使用 **类 Paxos 算法** 实现的强一致 MySQL 方案。

同时 Leaf 服务分 IDC 部署，服务调用的时候，根据负载均衡算法会优先调用同机房的 Leaf 服务。在该 IDC 内 Leaf 服务不可用的时候才会选择其他机房的 Leaf 服务。

Leaf-snowflake 方案完全沿用 Snowflake 方案的 bit 位设计。对于 workerID 的分配，当服务集群数量较小的情况下，完全可以手动配置。Leaf 服务规模较大，动手配置成本太高。所以使用 Zookeeper 持久顺序节点的特性自动对 Snowflake 节点配置 wokerID。

![image-20240616153216091](https://gitee.com/fushengshi/image/raw/master/image-20240616153216091.png)





# 附录

## Gossip 协议

Gossip 协议是一种允许在分布式系统中共享状态的**去中心化通信协议**，通过这种通信协议，可以将信息传播给网络或集群中的所有成员。这是一种随机且带有传染性的方式将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致。

> 最终一致性协议。

### 消息传播模式

- 反熵

  反熵（Anti-entropy）实际应用场景中，一般不会采用随机的节点进行反熵，而是可以设计成一个闭环。

  集群中的节点，每隔段时间就随机选择某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异，实现数据的最终一致性。

- 谣言传播

  谣言传播（Rumor mongering）指的是分布式系统中的一个节点一旦有了新数据之后，就会变为活跃节点，活跃节点会周期性地联系其他节点向其发送新数据，直到所有的节点都存储了该新数据。

### 实际应用

NoSQL 数据库 Redis 和 Apache Cassandra、服务网格解决方案 Consul 等知名项目都用到了 Gossip 协议。

Redis Cluster 内部的各个 Redis 节点通过Gossip 协议互相探测健康状态，在故障时可以⾃动切换。

- MEET

  在 Redis Cluster 中的某个 Redis 节点上执⾏ `CLUSTER MEET ip port` 命令，可以向指定的 Redis 节点发送⼀条 MEET 信息，⽤于将其添加进 Redis Cluster 成为新的 Redis 节点。

- PING/PONG

  Redis Cluster 中的节点都会定时地向其他节点发送 PING 消息，来交换各个节点状态信息，检查各个节点状态，包括在线状态、疑似下线状态 PFAIL 和已下线状态 FAIL。

- FAIL

  Redis Cluster 中的节点 A 发现 B 节点 PFAIL ，并且在下线报告的有效期限内集群中半数以上的节点将 B 节点标记为 PFAIL，节点 A 就会向集群⼴播⼀条 FAIL 消息，通知其他节点将故障节点 B 标记为 FAIL 。






















