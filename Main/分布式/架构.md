# 架构

## 域名系统 DNS

DNS（Domain Name System）的作用是把便于人类理解的域名地址，转换为便于计算机处理的 IP 地址。

- 权威域名服务器（Authoritative DNS）

  指负责翻译特定域名的 DNS 服务器，服务器决定了这个域名应该翻译出怎样的结果。

- 根域名服务器（Root DNS）

  指固定的、无需查询的顶级域名（Top-Level Domain）服务器，可以默认为它们已内置在操作系统代码之中。全世界一共有 13 组根域名服务器，之所以有 13 这个数字的限制是因为，DNS 主要是采用 UDP 传输协议来进行数据交换的，未分片的 UDP 数据包在 IPv4 下最大有效值为 512 字节，最多可以存放 13 组地址记录。

域名解析结果：

- A 记录，返回的是域名对应的 IP 地址。
- CNAME 记录，返回的是另一个域名，当前域名的解析跳转到另一个域名的解析上。

解析步骤：

- 检查本机的 hosts 文件。

- 请求本地 DNS（Local DNS，运营商提供的 DNS）

  按照 www.baidu.com 权威服务器、 baidu.com 权威服务器、com 的权威服务器依次查询地址记录。

- DNS 迭代查询。

  - 请求Root DNS，Root DNS返回顶级 DNS（com）的地址。

  - 请求 com 顶级 DNS，得到 baidu.com 的域名服务器地址。

  - 从 baidu.com 的域名服务器中查询 www.baidu.com 对应的 IP 地址，返回这个 IP 地址并标记来自于权威 DNS 的结果，写入 Local DNS 的解析结果缓存。



## 内容分发网络 CDN

内容分发网络（CDN，Content Distribution Network 或 Content Delivery Network），内容分发网络的工作过程，主要涉及到路由解析、内容分发、负载均衡和它所能支持的应用内容四个方面。

GSLB（Global Server Load Balance，全局负载均衡）可以 通过多种策略，来保证返回的 CDN 节点和用户尽量保证在同一地缘区域，比如说可以将用户的 IP 地址按照地理位置划分为若干的区域，然后将 CDN 节点对应到一个区域上，然后根据用户所在区域来返回合适的节点。也可以通过发送数据包测量 RTT 的方式来决定返回哪一个节点。

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240721125210042.png" alt="image-20240721125210042" style="zoom:80%;" />

- 路由解析

  内容分发网络将用户请求路由到它的资源服务器上，其实就是依靠 DNS 服务器来实现的。

  CNAME 记录在 DNS 解析过程中充当一个中间代理层的角色，可以把将用户最初使用的域名代理到正确的 IP 地址上。

- 内容分发

  - 主动分发就是由源站主动发起，将内容从源站或者其他资源库推送到用户边缘的各个 CDN 缓存节点上。

  - 被动回源就是指由用户访问所触发的全自动、双向透明的资源缓存过程。当某个资源首次被用户请求的时候，CDN 缓存节点如果发现自己没有该资源，就会实时从源站中获取。

应用：

- 加速静态资源

- 安全防御

  在广义上可以把 CDN 看作是网站的堡垒机，源站只对 CDN 提供服务，然后由 CDN 来服务外界的其他用户，这样恶意攻击者就不容易直接威胁源站。CDN 对防御某些攻击手段，如 DDoS 攻击等尤其有效。

- 协议升级

  CDN 提供商同时对接 SSL 证书服务，可以实现源站是 HTTP 协议的，而对外开放的网站是基于 HTTPS 的。

  这样的做法也可以实现源站到 CDN 是 HTTP/1.x 协议，而 CDN 提供的外部服务是 HTTP/2 或 HTTP/3 协议。或者是实现源站是基于 IPv4 网络的，CDN 提供的外部服务支持 IPv6 网络等等。

- 状态缓存

  客户端直接根据缓存信息来判断目标网站的状态。而 CDN 不仅可以缓存源站的资源，还可以缓存源站的状态，比如源站的301/302转向就可以缓存起来，让客户端直接跳转。

- 修改资源

  CDN 可以在给用户返回资源的时候，修改它的任何内容，以实现不同的目的。

  比如说，可以对源站未压缩的资源自动压缩，并修改 Content-Encoding，以节省用户的网络带宽消耗；可以针对源站未启用客户端缓存的内容，加上缓存 Header，来自动启用客户端缓存；可以修改 CORS 的相关 Header，给源站不支持跨域的资源提供跨域能力，等等。

- 访问控制

  CDN 可以实现 IP 黑/白名单功能。

  比如根据不同的来访 IP 提供不同的响应结果、根据IP的访问流量来实现 QoS 控制、根据 HTTP 的 Referer 来实现防盗链等等。

- 注入功能

  CDN 可以在不修改源站代码的前提下，为源站注入各种功能。



## 配置中心

配置中心是微服务架构中的一个标配组件，核心的功能是配置项的存储和读取。

> 开源方案有携程开源的 Apollo，百度开源的 Disconf，360 开源的 QConf，Spring Cloud 的组件 Spring Cloud Config 等等。

- Disconf、Apollo 使用 MySQL。
- QConf 使用 ZooKeeper。

配置信息存储之后，实现配置的动态变更：

- 轮询查询

  应用程序向配置中心客户端注册一个监听器，配置中心的客户端定期查询所需要的配置是否有变化，如果有变化则通知触发监听器，让应用程序得到变更通知。

- 长连推送

  在配置中心服务端保存每个连接关注的配置项列表。当配置中心感知到配置变化后通过这个长连接，把变更的配置推送给客户端。



## 服务发现

- 服务的注册（Service Registration）

  当服务启动的时候，它应该通过某些形式（比如调用 API、产生事件消息、在 ZooKeeper 或 Etcd 的指定位置记录、存入数据库等等）把自己的坐标信息通知给服务注册中心，这个过程可能由应用程序来完成（比如 Spring Cloud 的 @EnableDiscoveryClient 注解），也可能是由容器框架（比如 Kubernetes）来完成。

- 服务的维护（Service Maintaining）

  尽管服务发现框架通常都有提供下线机制，但并没有什么办法保证每次服务都能优雅地下线（Graceful Shutdown），而不是由于宕机、断网等原因突然失联。所以，服务发现框架就必须要自己去保证所维护的服务列表的正确性，以避免告知消费者服务的坐标后得到的服务却不能使用。

  现在的服务发现框架，一般都可以支持多种协议（HTTP、TCP等）、多种方式（长连接、心跳、探针、进程状态等）来监控服务是否健康存活，然后把不健康的服务自动下线。

- 服务的发现（Service Discovery）

  这里所说的发现是狭义的，它特指消费者从服务发现框架中，把一个符号（比如 Eureka 中的 ServiceID、Nacos 中的服务名、或者通用的 FDQN）转换为服务实际坐标的过程，这个过程现在一般是通过 HTTP API 请求，或者是通过 DNS Lookup 操作来完成（还有一些相对少用的方式，如 Kubernetes 也支持注入环境变量）。

服务发现是整个系统中，其他所有服务都直接依赖的最基础的服务（配置中心相同待遇，现在服务发现框架同时提供配置中心的功能），几乎没有办法在业务层面进行容错处理。而服务注册中心一旦崩溃，整个系统都会受到波及和影响，因此我们必须尽最大可能，在技术层面上保证系统的可用性。

直接以服务发现、服务注册中心为目标，或者间接用来实现这个目标的方式主要有以下三类：

- 基础设施（主要是指 DNS 服务器）

  这类的代表是 SkyDNS、CoreDNS。

  SkyDNS 的工作原理是从 API Server 中监听集群服务的变化，然后根据服务生成 NS、SRV 等 DNS 记录存放到 Etcd 中，kubelet 会在每个 Pod 内部设置 DNS 服务的地址作为 SkyDNS 的地址，在需要调用服务时，只需查询 DNS 把域名转换成 IP 列表便可实现分布式的服务发现。

- 分布式K/V存储框架

  这类的代表是 ZooKeeper、Doozerd、Etcd。这些 K/V 框架提供了分布式环境下读写操作的共识保证，都是 CP 的。

  ZooKeeper 通过可线性化（Linearizable）写入、全局 FIFO 顺序访问等机制来保障数据一致性。多节点部署的情况下， ZooKeeper 集群处于 Quorum 模式（大多数节点必须同意任何变更才能被视为有效）。

  Quorum 模式下的读请求不会触发各个 ZooKeeper 节点之间的数据同步，因此在某些情况下还是可能会存在读取到旧数据的情况，导致不同的客户端视图上看到的结果不同，这可能是由于网络延迟、丢包、重传等原因造成的。ZooKeeper 为了解决这个问题，提供了 Watcher 机制和版本号机制来帮助客户端检测数据的变化和版本号的变更，以保证数据的一致性。

  > Etcd 采用 Raft 算法。
  >
  > ZooKeeper 采用的是 ZAB 算法（一种 Multi Paxos 的派生算法）。

- 服务发现框架和工具

  这类的代表是 Eureka、Consul 和 Nacos。

  > Consul 采用 Raft 协议（CP），要求多数派节点写入成功后，服务的注册或变动才算完成，这就严格地保证了在集群外部读取到的服务发现结果一定是一致的。
  >
  > Eureka 的各个节点间采用异步复制（AP）来交换服务注册信息，服务注册或变动时，并不需要等待信息在其他节点复制完成，而是马上在该服务发现节点就宣告可见，其他节点是否可见并不保证。
  >
  > Nacos 不仅支持 CP 也支持 AP。采用类 Raft 协议实现 CP，采用自研的 Distro 协议实现 AP。



## 负载均衡

负载均衡指的是将用户请求分摊到不同的服务器上处理，以提高系统整体的并发处理能力以及可靠性。

真正的大型系统的负载均衡过程往往是多级的。比如在各地建有多个机房，或者是机房有不同网络链路入口的大型互联网站，然后它们会从 DNS 解析开始，通过 域名、CNAME、负载调度服务、就近的数据中心入口的路径，先根据 IP 地址（或者其他条件）将来访地用户分配到一个合适的数据中心当中，然后到负载均衡。

### 服务端负载均衡

![image-20240707212724639](https://gitee.com/fushengshi/image/raw/master/image-20240707212724639.png)

服务端负载均衡主要应用在系统外部请求和网关层之间，可以使用软件或者硬件实现。硬件负载均衡通过专门的硬件设备（比如 F5、A10、Array）实现负载均衡功能。软件负载均衡通过软件（比如 LVS、Nginx、HAproxy）实现负载均衡功能。

![image-20240707215131501](https://gitee.com/fushengshi/image/raw/master/image-20240707215131501.png)

- 四层负载均衡

  四层的意思是说，这些工作模式的共同特点是都维持着同一个 TCP 连接，而不是说它就只工作在第四层。事实上，这些模式主要都是工作在二层（数据链路层，改写 MAC 地址）和三层上（网络层，改写 IP 地址）。

  > 单纯只处理第四层（传输层，可以改写 TCP、UDP 等协议的内容和端口）的数据无法做到负载均衡的转发，因为 OSI 的下面三层是媒体层（Media Layers），上面四层是主机层（Host Layers）。既然流量已经到达目标主机上了，也就谈不上什么流量转发，最多只能做代理了。

  - 数据链路层负载均衡

    数据链路层负载均衡所做的工作是修改请求的数据帧中的 MAC 目标地址，让用户原本是发送给负载均衡器的请求的数据帧，被二层交换机根据新的 MAC 目标地址，转发到服务器集群中，对应的服务器的网卡上，这样真实服务器就获得了一个原本目标并不是发送给它的数据帧。

  - 网络层负载均衡

    这种负载均衡的模式称为 NAT 模式（Network Address Translation，NAT）。

    第一种保持原来的数据包不变：新创建一个数据包，把原来数据包的 Headers 和 Payload 整体作为另一个新的数据包的 Payload，在这个新数据包的 Headers 中，写入真实服务器的IP作为目标地址，然后把它发送出去。

    第二种改变目标数据包的方式：直接把数据包 Headers 中的目标地址改掉，修改后原本由用户发给均衡器的数据包，也会被三层交换机转发送到真实服务器的网卡上，而且因为没有经过 IP 隧道的额外包装，也就无需再拆包了。

- 七层负载均衡

  应用层主要协议是 HTTP，负载均衡比传输层负载均衡路由网络请求的方式更加复杂，它会读取报文的数据部分（比如 HTTP 部分的报文），然后根据读取到的数据内容（如 URL、Cookie）做出负载均衡决策。也就是说，应用层负载均衡器的核心是报文内容（如 URL、Cookie）层面的负载均衡，执行应用层负载均衡的设备通常被称为**反向代理服务器**。

  > Nginx 就是最常用的反向代理服务器，它可以将接收到的客户端请求以一定的规则（负载均衡策略）均匀地分配到这个服务器集群中所有的服务器上。

  - 所有 CDN 可以做的缓存方面的工作（除去 CDN 根据物理位置就近返回这种优化链路的工作外），七层负载均衡器全都可以实现，比如静态资源缓存、协议升级、安全防护、访问控制等等。

  - 实现更智能化的路由。

    比如根据 Session 路由，以实现亲和性的集群；根据 URL 路由，实现专职化服务（此时就相当于网关的职责）；根据用户身份路由，实现对部分用户的特殊服务（如某些站点的贵宾服务器）等等。

  - 某些安全攻击可以由七层负载均衡器来抵御。

    比如一种常见的 DDoS 手段是 SYN Flood 攻击，即攻击者控制众多客户端，使用虚假 IP 地址对同一目标大量发送 SYN 报文。从技术原理上看，因为四层负载均衡器无法感知上层协议的内容，这些 SYN 攻击都会被转发到后端的真实服务器上，而在七层负载均衡器下，这些 SYN 攻击会在负载均衡设备上被过滤掉，不会影响到后面服务器的正常运行。

  - 很多微服务架构的系统中，链路治理措施都需要在七层中进行，比如服务降级、熔断、异常注入等等。



### 客户端负载均衡

客户端负载均衡主要应用于系统内部的不同的服务之间，可以使用现成的负载均衡组件来实现。客户端负载均衡器和服务运行在同一个进程或者说 Java 程序里，不存在额外的网络开销。

Java 领域主流的微服务框架 Dubbo、Spring Cloud 等都内置了开箱即用的客户端负载均衡实现。Dubbo 属于是默认自带了负载均衡功能，Spring Cloud 是通过组件的形式实现的负载均衡。

Ribbon 支持的 7 种负载均衡策略：

- RandomRule：

  随机策略。

- RoundRobinRule（默认）：

  轮询策略。

- WeightedResponseTimeRule：

  权重（根据响应时间决定权重）策略。

- BestAvailableRule：

  最小连接数策略。

- RetryRule：

  重试策略（按照轮询策略来获取服务，如果获取的服务实例为 null 或已经失效，则在指定的时间之内不断地进行重试来获取服务，如果超过指定时间依然没获取到服务实例则返回 null）。

- AvailabilityFilteringRule：

  可用敏感性策略（先过滤掉非健康的服务实例，然后再选择连接数较小的服务实例）。

- ZoneAvoidanceRule：

  区域敏感性策略（根据服务所在区域的性能和服务的可用性来选择服务实例）。

Spring Cloud Load Balancer 支持的 2 种负载均衡策略：

- RandomLoadBalancer：

  随机策略。

- RoundRobinLoadBalancer（默认）：

  轮询策略。

Spring Cloud Load Balancer 支持的负载均衡策略其实不止这两种，ServiceInstanceListSupplier 的实现类同样可以让其支持类似于 Ribbon 的负载均衡策略。



### 负载均衡算法

- 随机法

- 轮询法

  如果没有配置权重的话，每个请求按时间顺序逐一分配到不同的服务器处理。如果配置权重的话，权重越高的服务器被访问的次数就越多。

- 哈希法

  将请求的参数信息通过哈希函数转换成一个哈希值，然后根据哈希值来决定请求被哪一台服务器处理。

- 一致性哈希

  一致性哈希法的核心思想是将数据和节点都映射到一个哈希环上，然后根据哈希值的顺序来确定数据属于哪个节点。当服务器增加或删除时，只影响该服务器的哈希，而不会导致整个服务集群的哈希键值重新分布。

- 最小连接法

- 最小活跃法

- 最快响应时间法



## 网关路由

微服务中网关的首要职责，就是以统一的地址对外提供服务，将外部访问这个地址的流量，根据适当的规则路由到内部集群中正确的服务节点之上。

微服务的**网关首先应该是个路由器**，在满足此前提的基础上，网关还可以根据需要作为流量过滤器来使用，以提供某些额外的可选的功能。比如安全、认证、授权、限流、监控、缓存等等。

> 网关 = 路由器（基础职能） + 过滤器（可选职能）

对比负载均衡：

- 从技术实现角度

  对于路由流量这项工作，负载均衡器与服务网关的实现是没有什么差别的，很多服务网关本身就是基于老牌的负载均衡器来实现的，比如 Nginx、HAProxy 对应的 Ingress Controller 等等。

- 从路由目的角度

  负载均衡器与服务网关的区别在于，前者是为了根据均衡算法对流量进行平均地路由，后者是为了根据流量中的某种特征进行正确地路由。



框架：

- Zuul

  Zuul 是 Netflix 开发的一款提供动态路由、监控、弹性、安全的网关服务，基于 Java 技术栈开发，可以和 Eureka、Ribbon、Hystrix 等组件配合使用。

  在 Zuul 1.0 时，它采用的是阻塞 I/O 模型，来进行最经典的 一条线程对应一个连接（Thread-per-Connection）的方式来代理流量，而采用阻塞 I/O 就意味着它会有线程休眠，就有上下文切换的成本。

  Zuul 的 2.0 版本，最大的改进就是基于 Netty Server 实现了异步 I/O 模型来处理请求，大幅度减少了线程数，获得了更高的性能和更低的延迟。

  <img src="https://gitee.com/fushengshi/image/raw/master/image-20240616155304799.png" alt="image-20240616155304799" style="zoom:80%;" />

- SpringCloud Gateway

  SpringCloud Gateway 属于 Spring Cloud 生态系统中的网关。

  为了提升网关的性能，SpringCloud Gateway 基于 Spring WebFlux 。Spring WebFlux 使用 Reactor 库来实现响应式编程模型，底层基于 Netty 实现同步非阻塞的 I/O。Spring Cloud Gateway 不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，限流。

  ![image-20240616155537114](https://gitee.com/fushengshi/image/raw/master/image-20240616155537114.png)





## 服务容错

容错策略：

- 快速失败（Failfast）

  尽快让服务报错并抛出异常，坚决避免重试，由调用者自行处理。

  > 断路器模式，服务熔断。

- 安全失败（Failsafe）

  在一个调用链路中的服务通常也有主路和旁路之分，属于旁路逻辑的一个显著特点是服务失败了也不影响核心业务的正确性。比如基于 Spring 管理的应用程序，通过扩展点、事件或者 AOP 注入的逻辑往往就属于旁路逻辑，典型的有审计、日志、调试信息等等。

  即使旁路逻辑调用失败了也当作正确来返回，如果需要返回值的话，系统就自动返回一个符合要求的数据类型的对应零值，然后自动记录一条服务调用出错的日志备查。

  > 服务降级。

- 故障转移（Failover）

  如果调用的服务器出现故障，系统不会立即向调用者返回失败结果，而是**自动切换到其他服务副本**（如主备切换），尝试其他副本能否返回成功调用的结果，从而保证了整体的高可用性。

  故障转移策略能够实施的前提，是服务具有幂等性。

  > 重试模式。

- 故障恢复（Failback）

  故障恢复是指，当服务调用出错了以后，将该次调用失败的信息存入一个消息队列中，然后由系统自动开始**异步重试调用**。

  > 重试模式。

- 沉默失败（Failsilent）

  如果大量的请求需要等到超时（或者长时间处理后）才宣告失败，很容易因为某个远程服务的请求堆积而消耗大量的线程、内存、网络等资源，进而影响到整个系统的稳定性。

  一种合理的失败策略是当请求失败后，就默认服务提供者一定时间内无法再对外提供服务，不再向它分配请求流量，并将错误隔离开来，避免对系统其他部分产生影响。

  > 舱壁隔离模式。

- 并行调用（Forking）

  并行调用策略，是指一开始就同时向多个服务副本发起调用，只要有其中任何一个返回成功，那调用便宣告成功。这种策略是在一些关键场景中，使用更高的执行成本换取执行时间和成功概率的策略。

- 广播调用（Broadcast）

  广播调用与并行调用是相对应的，都是同时发起多个调用，但并行调用是任何一个调用结果返回成功便宣告成功，而广播调用则是要求所有的请求全部都成功，才算是成功。

容错设计模式：

- 断路器模式（熔断）

  通过断路器对远程服务进行熔断，就可以避免因为持续的失败或拒绝而消耗资源，因为持续的超时而堆积请求，最终可以避免雪崩效应的出现。断路器本质上是快速失败策略的一种实现方式。

  在快速失败策略明确反馈了故障信息给上游服务以后，上游服务必须能够主动处理调用失败的后果，而不是坐视故障扩散。

  > 服务的熔断机制是指：在服务 A 调用服务 B 时，如果 B 返回错误或超时的次数超过一定阈值，服务 A 的后续请求将不再调用服务 B。这种设计方式就是断路器模式。
  >
  > 降级设计本质上是站在系统整体可用性的角度上考虑问题：当资源和访问量出现矛盾时，在有限的资源下，放弃部分非核心功能或者服务，保证整体的可用性，这是一种有损的系统容错方式。

- 舱壁隔离模式

  舱壁隔离模式，是常用的实现服务隔离的设计模式。

- 重试模式

  故障转移和故障恢复这两种策略都需要对服务进行重复调用。



## 流量控制

- 每秒事务数（Transactions per Second，TPS）

  TPS 是衡量信息系统吞吐量的最终标准。事务 可以理解为一个逻辑上具备原子性的业务操作。

- 每秒请求数（Hits per Second，HPS）

  HPS 是指每秒从客户端发向服务端的请求数。

  主流系统大多倾向于使用 HPS 作为首选的限流指标，因为它相对容易观察统计，而且能够在一定程度上反映系统当前以及接下来一段时间的压力。

- 每秒查询数（Queries per Second，QPS）

  QPS 是指一台服务器能够响应的查询次数。如果只有一台服务器来应答请求，那 QPS 和 HPS 是等价的，但在分布式系统中，一个请求的响应往往要由后台多个服务节点共同协作来完成。

### 限流设计模式

- 流量计数器

  只是针对时间点进行离散的统计。

- 滑动窗口

  滑动时间窗口模式的限流完全解决了流量计数器的缺陷，它可以保证在任意时间片段内，只需经过简单的调用计数比较，就能控制住请求次数一定不会超过限流的阈值，在单机限流或者分布式服务单点网关中的限流中很常用。

  > 滑动窗口算法（Sliding Window Algorithm）在计算机科学的很多领域中都有成功的应用，比如编译原理中的窥孔优化（Peephole Optimization）、TCP 协议的阻塞控制（Congestion Control）等都使用到了滑动窗口算法。

- 漏桶

  漏桶在代码实现上其实就是一个以请求对象作为元素的先入先出队列（FIFO Queue），队列长度就相当于漏桶的大小，当队列已满时就拒绝新的请求进入。

- 令牌桶

  漏桶是从水池里往系统出水，令牌桶则是系统往排队机中放入令牌。

  令牌桶模式的实现看似可能比较复杂，每间隔固定时间，我们就要把新的令牌放到桶中，但其实我们并不需要真的用一个专用线程或者定时器来做这件事情，只要在令牌中增加一个时间戳记录，每次获取令牌前，比较一下时间戳与当前时间，可以计算出这段时间需要放多少令牌进去，然后一次性放全部令牌即可。

  > Guava 中的限流工具类 RateLimiter 就是令牌桶的一个实现。

### 分布式限流

分布式限流的目的是要让各个服务节点的协同限流。无论是将限流功能封装为专门的远程服务，还是在系统采用的分布式框架中有专门的限流支持，都需要把每个服务节点的内存中的统计数据给开放出来，让全局的限流服务可以访问到。

- 基于中心化的限流方案

  通过中心化的组件（如 Redis），可以设置每秒钟允许的最大请求数（QPS），并将这个值存储在 Redis 中。对于每个请求，服务器需要先向 Redis 请求令牌。如果获取到令牌说明请求可以被处理，如果没有获取到令牌说明请求被限流，可以返回一个错误信息或者稍后重试。

  Redis成为整个系统的性能瓶颈，如果 Redis 出现故障，整个系统的限流功能将受到影响。Redis 是一个基于网络通信的内存数据库，因此网络带宽是其性能的一个关键因素。

- 基于负载均衡的限流方案

  使用负载均衡器或分布式服务发现（如北极星），将请求均匀地分发到每个机器上。确保每个机器都能处理一部分请求。在每个机器上维护本机的限流状态，实现本地缓存单机限流的逻辑。

  本地缓存单机限流的精度受限于每个服务实例的资源和配置，这可能导致限流策略无法精确地适应整个系统的流量变化，无法灵活地调整限流规则。

- 基于分布式协调服务的限流方案

  使用 ZooKeeper 或者 etcd 等分布式协调服务来实现限流。在 ZooKeeper 中创建一个节点，节点的数据代表令牌的数量，当一个请求到达时，服务器首先向 ZooKeeper 申请一个令牌。

  优点是可以实现精确的全局限流，可以避免单点故障。但是这个方案的缺点是实现复杂，且对 ZooKeeper 的性能有较高的要求。



## 安全设计

### 认证

- 通讯信道上的认证

  建立通讯连接之前，要先证明你是谁。在网络传输（Network）场景中的典型是基于SSL/TLS传输安全层的认证。

- 通讯协议上的认证

  请求获取资源之前，要先证明你是谁。在互联网（Internet）场景中的典型是基于HTTP协议的认证。

  Basic 认证产生用户身份凭证的方法是让用户输入用户名和密码，经过 Base64 编码后作为身份凭证。

  > HTTP Basic认证是一种以演示为目的的认证方案，在一些不要求安全性的场合也有实际应用。

- 通讯内容上的认证

  你使用我提供的服务之前，要先证明你是谁。在万维网（World Wide Web）场景中的典型是基于Web内容的认证。

  依靠内容而不是传输协议来实现的认证方式，在万维网里就被称为Web认证，由于在实现形式上，登录表单占了绝对的主流，因此它通常也被称为表单认证（Form Authentication）。

  WebAuthn彻底抛弃了传统的密码登录方式，改为直接采用生物识别（指纹、人脸、虹膜、声纹）或者实体密钥（以 USB、蓝牙、NFC 连接的物理密钥容器）来作为身份凭证，从根本上消灭了用户输入错误产生的校验需求，以及防止机器人模拟产生的验证码需求等问题。

### 授权

OAuth 2.0 是面向于解决第三方应用（Third-Party Application）的认证授权协议，以令牌（Token）代替用户密码作为授权的凭证。OAuth 2.0 这种授权协议，就是保证第三方（软件）只有在获得授权之后，才可以进一步访问授权者的数据。

在RBAC（基于角色的访问控制，Role-Based Access Control）模型中，角色拥有许可的数量，是根据完成该角色工作职责所需的最小权限所赋予的。

> 在 Spring Security 的设计里，用户和角色都可以拥有权限，比如在它的 HttpSecurity 接口就同时有着 `hasRole()` 和 `hasAuthority()` 方法。

### 凭证

![image-20240707223914127](https://gitee.com/fushengshi/image/raw/master/image-20240707223914127.png)

#### Cookie-Session

以键值对的方式向客户端发送一组信息，在此后一段时间内的每次 HTTP 请求中，这组信息会附带着名为 Cookie 的 Header 重新发回给服务端，以便服务器区分来自不同客户端的请求。

- Cookie

  由于 Cookie 是放在请求头上的，属于额外的传输负担，不应该携带过多的内容，而且放在 Cookie 中传输也并不安全，容易被中间人窃取或篡改。

- Session

  系统把状态信息保存在服务端，在 Cookie 里只传输一个无字面意义的、不重复的字符串，通常习惯上是以 sessionid 或者 jsessionid 为名。

Cookie-Session 在单节点的单体服务环境中确实是最合适的方案，当服务器水平拓展成多节点时：

- 牺牲集群的一致性（Consistency）

  让均衡器采用亲和式的负载均衡算法。比如根据用户IP或者 Session 来分配节点，每一个特定用户发出的所有请求，都一直被分配到其中某一个节点来提供服务，每个节点都不重复地保存着一部分用户的状态。

  如果保存节点崩溃了，用户状态便完全丢失。

- 牺牲集群的可用性（Availability）

  让各个节点之间采用复制式的 Session，每一个节点中的 Session 变动，都会发送到组播地址的其他服务器上。即使某个节点崩溃了，也不会中断某个用户的服务。

  Session之间组播复制的同步代价比较高昂，节点越多时，同步成本就越高。

- 牺牲集群的分区容错性（Partition Tolerance）

  让普通的服务节点中不再保留状态，将上下文集中放在一个所有服务节点都能访问到的数据节点中进行存储。

  数据节点就成为了单点，一旦数据节点损坏或出现网络分区，整个集群都不能再提供服务。

#### JWT

JWT（JSON Web Token）是目前广泛使用的一种令牌格式，尤其经常与 OAuth 2.0 配合应用于分布式的、涉及多方的应用系统中。

- 令牌头（Header）

  ```json
  {"alg": "HS256", "typ": "JWT"}
  ```

  描述了令牌的类型以及令牌签名的算法，示例中 HS256 为 HMAC SHA256 算法的缩写。

  > JWT 默认的签名算法 HMAC SHA256 是一种带密钥的哈希摘要算法，加密与验证过程都只能由中心化的授权服务来提供，所以这种方式一般只适合于授权服务与应用服务处于同一个进程中的单体应用。
  >
  > 在多方系统，或者是授权服务与资源服务分离的分布式应用当中，通常会采用非对称加密算法来进行签名。这时候，除了授权服务端持有的可以用于签名的私钥以外，还会对其它服务器公开一个公钥。这个公钥不能用来签名，但它能被其他服务用于验证签名是否由私钥所签发的。

- 负载（Payload）

  令牌真正需要向服务端传递的信息。

- 签名（Signature）

  使用在对象头中公开的特定签名算法，通过特定的密钥（Secret，由服务器进行保密）对前面两部分内容进行加密计算，产生签名值。

  ```c
  HMACSHA256(base64UrlEncode(header) + "." + base64UrlEncode(payload) , secret)
  ```


缺点：

- 令牌难以主动失效

  采用 JWT，就必须设计一个黑名单的额外逻辑，把要主动失效的令牌集中存储起来。

  无论这个黑名单是实现在 Session、Redis 还是数据库当中，都会让服务退化成有状态服务，降低了 JWT 本身的价值。维护的黑名单一般是很小的状态量，因此在许多场景中还是有存在价值的。

- 相对更容易遭受重放攻击

  建议的解决方案是在信道层次（比如启用 HTTPS）上解决，不提倡在服务层次（比如在令牌或接口其他参数上增加额外逻辑）上解决。

  > Cookie-Session 也是有重放攻击问题的，只是因为 Session 中的数据控制在服务端手上，应对重放攻击会相对主动一些。

- 必须考虑令牌在客户端如何存储

- 无状态也有缺点

### 保密

#### 客户端加密

为了保证信息不被黑客窃取而去做客户端加密，没有太大意义，对绝大多数的信息系统来说，启用 HTTPS 可以说是唯一的实际可行的方案。为了保证密码不在服务端被滥用，在客户端就开始加密的做法是有意义的。

> 中间人攻击（即攻击者），它是指通过劫持掉客户端到服务端之间的某个节点，包括但不限于代理（通过 HTTP 代理返回赝品）、路由器（通过路由导向赝品）、DNS 服务（直接将机器的 DNS 查询结果替换为赝品地址）等，给访问的页面或服务注入恶意的代码。

#### 服务端加密

一个普通安全强度的信息系统，密码从客户端传输到服务端，然后存储进数据库。在具有一定保密安全性的同时，避免消耗过多的运算资源。即使是用户采用了弱密码、客户端通讯被监听、服务端被拖库、泄露了存储的密文和盐值等问题同时发生，也能够最大限度地避免用户明文密码被逆推出来。

为了防御彩虹表攻击，进行加盐处理，客户端加盐只需要取固定的字符串即可。

```java
client_hash = MD5(MD5(password) + salt)
```

假设攻击者截获了客户端发出的信息，得到了摘要结果和采用的盐值，那攻击者就可以枚举遍历所有弱密码，然后对每个密码再加盐计算，就得到了一个针对固定盐值的对照彩虹表。为了应对这种暴力破解，引入慢哈希函数。

慢哈希函数是指这个函数的执行时间是可以调节的哈希函数，它通常是以控制调用次数来实现的。BCrypt 算法就是一种典型的慢哈希函数，它在做哈希计算时，接受盐值 Salt 和执行成本 Cost 两个参数。

```java
client_hash = BCrypt(MD5(password) + salt) 
```

防御服务端被拖库后，针对固定盐值的批量彩虹表攻击，具体做法是为每一个密码（指客户端传来的哈希值）产生一个随机的盐值。把随机生成的盐值混入客户端传来的哈希值，再做一次哈希，产生出最终的密文，并和随机生成的盐值一起写入到同一条数据库记录中。

```java
SecureRandom random = new SecureRandom();
byte server_salt[] = new byte[36];
random.nextBytes(server_salt);

server_hash = SHA256(client_hash + server_salt); 
DB.save(server_hash, server_salt);
```

### 传输

> 详见 HTTP.md 中 SSL/TLS 协议。



## 可观测性

可观测性一般会被分成三种具体的表现形式，分别是日志收集、链路追踪和聚合度量。

### 日志收集

日志的职责是记录离散事件，通过这些记录事后分析出程序的行为。

- 收集与缓冲

  覆盖整个链路的全局日志系统，当每个节点输出日志到文件后，就必须要把日志文件统一收集起来，集中存储、索引（这一步由 Elasticsearch 来负责），由此产生了专门的日志收集器。ELK（Elastic Stack）中的日志收集与加工聚合的职责，都是由 Logstash 来承担的，Logstash 既部署在各个节点中作为收集的客户端（Shipper），也同时有独立部署的节点作为归集转换日志的服务端（Master）。

  一种最常用的缓解压力的做法，是将日志接收者从 Logstash 和 Elasticsearch 转移至抗压能力更强的队列缓存。比如在Logstash之前，架设一个 Kafka 或者 Redis 作为缓冲层，当面对突发流量，Logstash 或 Elasticsearch 的处理能力出现瓶颈时，就自动削峰填谷，这样甚至当它们短时间停顿，也不会丢失日志数据。

- 加工与聚合

  在将日志集中收集之后，以及存入 Elasticsearch 之前，我们一般还要对它们进行加工转换和聚合处理，这一步通常就要使用 Logstash。

  从离散的日志中获得统计信息：

  - 通过 Elasticsearch 本身的处理能力做实时的聚合统计。这是一种很便捷的方式，不过要消耗 Elasticsearch 服务器的运算资源。

  - 在收集日志后自动生成某些常用的、固定的聚合指标，这种聚合在 Logstash 中通过聚合插件来完成。

- 存储与查询

  经过收集、缓冲、加工、聚合之后的日志数据，放入 Elasticsearch 中索引存储，Elasticsearch 是整个 Elastic Stack 技术栈的核心。

  日志的数据特征就决定了所有用于日志分析的 Elasticsearch，都会使用时间范围作为索引，比如根据实际数据量的大小，可能是按月、按周或者按日、按时。

  日志基本上只会以最近的数据为检索目标，随着时间推移，早期的数据会逐渐失去价值。这点就决定了我们可以很容易地区出分冷数据和热数据，进而对不同数据采用不一样的硬件策略。

### 链路追踪

从广义上讲，一个完整的分布式追踪系统，应该由数据收集、数据存储和数据展示三个相对独立的子系统构成。

从狭义上讲，特指链路追踪数据的收集部分。比如 Spring Cloud Sleuth 就属于狭义的追踪系统，通常会搭配Zipkin作为数据展示，搭配 Elasticsearch 作为数据存储来组合使用。

- 基于日志的追踪

  基于日志的追踪思路是将 Trace、Span 等信息直接输出到应用日志中，然后随着所有节点的日志归集过程汇聚到一起，再从全局日志信息中反推出完整的调用链拓扑关系。日志追踪对网络消息完全没有侵入性，对应用程序只有很少量的侵入性，对性能的影响也非常低。

  缺点是直接依赖于日志归集过程，日志本身不追求绝对的连续与一致导致了基于日志的追踪往往不如其他两种追踪实现来的精准。

- 基于服务的追踪

  服务追踪的实现思路是通过某些手段给目标应用注入追踪探针（Probe），比如针对 Java 应用，一般就是通过 Java Agent 注入的。

  > 基于服务的追踪是目前最为常见的追踪实现方式，Zipkin、SkyWalking、Pinpoint 等主流追踪系统广泛采用。
  >
  > 采用 traceId 和 spanId 这两个数据维度来记录服务之间的调用关系（traceId 就是 requestId），使用 traceId 串起单次请求，spanId 记录每一次 RPC 调用。

  基于服务的追踪会比基于日志的追踪消耗更多的资源，也具有更强的侵入性，而换来的收益就是追踪的精确性与稳定性都有所保证，不必依靠日志归集来传输追踪数据。

- 基于边车代理的追踪

  基于边车代理的追踪是服务网格的专属方案，也是最理想的分布式追踪模型。

  缺点是服务网格现在还不够普及。

### 聚合度量

度量（Metrics）的目的是揭示系统的总体运行状态。度量就是用经过聚合统计后的高维度信息，以最简单直观的形式来总结复杂的过程，为监控、预警提供决策支持。度量可以分为客户端的指标收集、服务端的存储查询、终端的监控预警三个相对独立的过程，每个过程在系统中一般也会设置对应的组件来实现。

虽然说 Prometheus 在度量领域的统治力，暂时还不如日志领域中 Elastic Stack 的统治地位那么稳固，但在云原生时代里它基本已经是事实标准了。

> 详见 Prometheus。



## 服务网格 Istio

ServiceMesh 采用基于 sidecar 通信代理，将基础的治理能力从服务框架中解耦。ServiceMesh 作为微服务架构中负责网络通信的基础设施层，可以对流量进行任意处理，一些主要的功能：

- 动态路由

  可通过路由规则来动态路由到所请求的服务，便于不同环境、不同版本等的动态路由调整。

- 故障注入

  通过引入故障来模拟网络传输中的问题（如延迟）来验证系统的健壮性，方便完成系统的各类故障测试。

- 熔断

  通过服务降级来终止潜在的关联性错误。

- 安全

  在服务网格上实现安全机制（如 TLS），并且很容易在基础设施层完成安全机制更新。

- 多语言支持

  作为独立运行且对业务透明的 Sideca 代理，很轻松地支持多语言的异构系统。 多协议支持。 同多语言一样，也支持多协议。

- 指标和分布式链路追踪。

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240720173322062.png" alt="image-20240720173322062" style="zoom:80%;" />

Istio 是由 Google、IBM 和 Lyft 发起的开源的 ServiceMesh 框架，由数据平面和控制平面组成。

- 数据平面：数据平面就是使用 envoy 作为 sidecar 的数据代理服务，数据平面也称为转发平面。

- 控制平面：控制平面用以配置、管理、下发数据平面的转发规则。



# 附录

## 可用性评估

服务等级协议（Service-Level Agreement，SLA）最根本的形式是协议双方（服务提供者和用户）签订的一个合约或协议。这个合约规范了双方的商务关系或部分商务关系。可以认为 SLA 是服务可用性一个重要衡量指标。

```
Availability = MTBF / (MTBF + MTTR)
```

- MTBF（Mean Time Between Failure）

  平均故障间隔的意思，代表两次故障的间隔时间，也就是系统正常运转的平均时间。

- MTTR（Mean Time To Repair）

  表示故障的平均恢复时间，也可以理解为平均故障时间。

业界一般用几个 9 的 SLA 服务等级来衡量互联网应用的可用性。

- 一般来讲，2 个 9 表示系统基本可用，年度不可用时间小于 88 小时。
- 3 个 9 是较高可用，年度不可用时间小于 9 个小时。
- 4 个 9 是具有自动恢复能力的高可用，年度不可用时间小于 53 分钟。
- 5 个 9 指极高的可用性，年度不可用时间小于 5 分钟。

仅凭理论指标在有些情况下是不能满足实际需求的，通过 **停机时间影响请求 / 总的请求量** 评估业务在高峰期停机和在低峰期停机分别造成多少的损失。



## 高性能

一般来讲，系统建立的会话数量就是用户同时访问系统的数量。可以通过公式，估算出系统的吞吐量（throughput）和延迟（latency）。

- 吞吐量（系统处理请求的速率）：反映单位时间内处理请求的能力（单位一般是 TPS 或 QPS）。

- 延迟（响应时间）：从客户端发送请求到接收响应的时间（单位一般是 ms、s）。

- TP（Top Percentile）

  TP 99 为例，它是指请求中 99% 的请求能达到的性能，TP 是一个时间值，比如 TP 99 = 10ms，含义就是 99% 的请求，在 10ms 之内可以得到响应。

![image-20240706163006934](https://gitee.com/fushengshi/image/raw/master/image-20240706163006934.png)

对一些延迟要求比较高的系统来说，系统优化性能指标是要找到延迟趋向最低和吞吐量趋向最高的点。



### 系统性能指标

![image-20240706163019253](https://gitee.com/fushengshi/image/raw/master/image-20240706163019253.png)

- DNS 解析

  用户在浏览器输入 URL 按回车，请求会进行 DNS 查找，浏览器通过 DNS 解析查到域名映射的IP 地址，查找成功后，浏览器会和该 IP 地址建立连接。对应的性能指标为：DNS 解析时间。

  - 通过 DNS 缓存或 DNS 预解析，适当增大域名的 TTL 值来增大 DNS 服务器缓存域名的时间，进而提升了缓存的命中率。

  - 可以用 dns-prefetch 标签实现域名的预解析，让浏览器在后台把要用的 DNS请求提前解析，当用户访问的网页中包含了预解析的域名时，再次解析 DNS 就不会有延迟了。

    ```bash
    <link rel="dns-prefetch" href="//static.360buyimg.com">
    ```

- 建立 TCP 连接

  HTTP 是基于 TCP 协议基础上进行数据传输的。建立 TCP 请求连接，可以用 TCP的连接时间来衡量浏览器与 Web 服务器建立的请求连接时间。

- 服务器响应

  服务器端的延迟和吞吐能力。针对影响服务端性能的指标，可以细分为基础设施性能指标、数据库性能指标，系统应用性能指标。

  - 基础设施性能指标主要针对 CPU 利用率、磁盘 I/O，网络带宽、内存利用率等。

    如果 CPU 占用率超过 80%，很可能是系统出了问题。如果内存利用率 100%，可能是因为内存中存放了缓存，因此还要衡量 SWAP 交换空间的利用率。另外，还要考虑容器的 JVM 的 Full GC 情况、磁盘 I/O 是否可以优化、网络带宽是否存在瓶颈等问题都会影响系统的最终性能。

  - 数据库的性能指标主要有 SQL 查询时间、并发数、连接数、缓存命中率等。

  - 系统应用性能指标和系统业务有关，因为业务场景影响架构设计，比如 ToC 的系统一般会设计成同步 RPC 调用，因为要实时反馈用户的请求，而 ToB 的系统则可以设计成事件驱动模式，通过异步通知的方式，推送或拉取数据，两种架构对比，异步事件驱动的吞吐量会更高。

- 白屏时间

  浏览器自上而下显示 HTML，同时渲染顺序也是自上而下的，所以当用户在浏览器地址栏输入 URL 按回车，到他看到网页的第一个视觉标志为止，这段白屏时间可以作为一个性能的衡量指标（白屏时间越长，用户体验越差）。

  - 减少首次文件的加载体积，比如用 gzip 算法压缩资源文件。
  - 调整用户界面的浏览行为，现在主流的 Feed 流也是一种减少白屏时间的方案。

- 首屏时间

  首屏时间是指：用户在浏览器地址输入 URL 按回车，然后看到当前窗口的区域显示完整页面的时间。

  用户端浏览界面的渲染，首屏时间也是一个重要的衡量指标。一般情况下，一个页面总的白屏时间在 2 秒以内，用户会认为系统响应快 2~5 秒，用户会觉得响应慢，超过 5 秒很可能造成用户流失。



## 监控系统

监控系统包括三个部分：基础设施监控报警、系统应用监控报警，以及存储服务监控报警。

- 基础设施监控

  监控报警指标分为两种类型：

  - 系统要素指标：主要有 CPU、内存、磁盘。

  - 网络要素指标：主要有带宽、网络 I/O、CDN、DNS、安全策略、和负载策略。

  > 监控工具：
  >
  > - ZABBIX：Alexei Vladishev 开源的监控系统，覆盖市场最多的老牌监控系统，资料很多。
  >
  > - Open-Falcon：小米开源的监控系统，小米、滴滴、美团等公司内部都在用。
  >
  > - Prometheus：SoundCloud 开源监控系统，对 K8S 的监控支持更好。

- 系统应用监控

  业务状态监控报警，关注点在于系统自身状态的监控报警。

  系统应用监控报警的核心监控指标主要有流量、耗时、错误、心跳、客户端数、连接数等 6 个核心指标。

  > 监控工具有 CAT、SkyWalking、Pinpoint、Zipkin 等，用于**链路追踪和监控**。
  >
  > - Zipkin 是一种开源的分布式链路追踪系统，可以用于监控和跟踪微服务架构中的请求调用链。

- 存储服务监控

  常用的第三方存储有 DB、ES、Redis、MQ 等。

  对于存储服务的监控，除了基础指标监控之外，还有一些比如集群节点、分片信息、存储数据信息等相关特有存储指标的监控。



## 秒杀场景

因为用户查询的是少量的商品数据属于查询的热点数据，可以采用缓存策略，将请求尽量挡在上层的缓存中。

- 商城里的图片和视频数据尽量做到静态化，这样就可以命中 CDN 节点缓存，减少 Web 服务器的查询量和带宽负担。
- Web 服务器比如 Nginx 可以直接访问分布式缓存节点，这样可以避免请求到达 Tomcat 等业务服务器。

在秒杀场景下，高并发的写请求并不是持续的，可以将秒杀请求暂存在消息队列中，然后业务服务器会响应用户 秒杀结果正在计算中，释放了系统资源之后再处理其它用户的请求。在后台启动若干个队列处理程序，消费消息队列中的消息，再执行校验库存、下单等逻辑。因为只有有限个队列处理线程在执行，所以落入后端数据库上的并发请求是有限的。而请求是可以在消息队列中被短暂地堆积，当库存被消耗完之后，消息队列中堆积的请求就可以被丢弃了。

> 消息队列在秒杀系统中最主要的作用：削峰填谷。

整个的购买流程，有主要的业务逻辑，也会有次要的业务逻辑：比如主要的流程是生成订单、扣减库存，次要的流程可能是我们在下单购买成功之后会给用户发放优惠券，会增加用户的积分。将发放优惠券、增加积分的操作放在另外一个队列处理机中执行。

> 消息队列可以实现异步处理简化秒杀请求中的业务流程，提升系统的性能。

秒杀系统产生一条购买数据后，可以先把全部数据发送给消息队列，然后数据团队再订阅这个消息队列的 Topic，当数据团队的接口发生故障不会影响到秒杀系统的可用性。

> 消息队列可以解耦合提升整体系统的鲁棒性。



## Prometheus

![image-20240715000117779](https://gitee.com/fushengshi/image/raw/master/image-20240715000117779.png)

- Prometheus Server：Prometheus 的核心模块，主要是采集和存储各个时序指标数据，并提供查询的功能，其中存储包括本地存储和远程存储。

- Pushgateway：通常 Prometheus 采用拉取模式，但是也提供推模式，可以推送数据到 pushgateway 来实现和拉取数据一样的效果，比如一些定时任务等临时作业。

- Exporter：Exporter 是 Prometheus 的监控对象，主要是收集数据，比如 golang 程序中导出的 `/metrics` 数据，官方也提供很多类型的 Exporter，比如 CPU，内存等采集。

- Service Discovery：在云原生监控中，找到对应的 Exporter 不可能手动配置，所以 Prometheus 提供了服务发现机制来采集Exporter 节点，支持 DNS，Consule，文件和 k8s API 等方式。

- Dashboard：Prometheus 提供的 Web UI，虽然通常与 Grafana 组合使用，但是独立的 Web UI 可以方便快速查询和展示图标。

- Alertmanager：独立于 Prometheus 的告警组件，是独立的服务部署，Alertmanager 主要接收 Prometheus 推送过来的告警，用于管理、整合和分发告警信息，除了这些还提供分组，抑制和静默等告警特性。

### 客户端指标收集

指标采集方式有两种解决方案：拉取式采集（Pull-Based Metrics Collection）和推送式采集（Push-Based Metrics Collection）。

> 老牌的度量系统比如 [Ganglia](https://en.wikipedia.org/wiki/Ganglia_(software))、[Graphite](https://graphiteapp.org/)、[StatsD](https://github.com/statsd/statsd) 等采用推送式采集。
>
> 以 Prometheus、[Datadog](https://www.datadoghq.com/pricing/)、[Collectd](https://en.wikipedia.org/wiki/Collectd) 为代表的度量系统采用拉取式采集。

Prometheus 在基于 Pull 架构的同时，还能够有限度地兼容 Push 式采集，一个位于 Prometheus Server 外部的相对独立的中介模块会把外部推送来的指标放到 Push Gateway 中暂存，然后再等候 Prometheus Server 从 Push Gateway 中去拉取。

> Prometheus 设计 Push Gateway 的本意是为了解决 Pull 的一些固有缺陷：
>
> 比如目标系统位于内网，需要通过 NAT 访问外网，而外网的 Prometheus 是无法主动连接目标系统的，这就只能由目标系统主动推送数据。
>
> 比如某些小型短生命周期服务可能还等不及 Prometheus 来拉取，服务就已经结束，只能由服务自己 Push 来保证度量的及时和准确。

### 服务端存储查询

Prometheus 服务端自己就内置了一个强大的**时序数据库**实现，这个时序数据库提供了一个名为 PromQL 的数据查询语言，能对时序数据进行丰富的查询、聚合以及逻辑运算。

### 监控预警

界面分析和监控预警是与用户更加贴近的功能模块，对度量系统本身而言，它们都属于相对外围的功能。

Prometheus 提供了专门用于预警的 Alert Manager，将 Alert Manager 与 Prometheus 关联后，可以设置某个指标在多长时间内、达到何种条件就会触发预警状态，在触发预警后，Alert Manager 就会根据路由中配置的接收器，比如邮件接收器、微信接收器或者更通用的 WebHook 接收器等自动通知。

> 见监控系统的基础设施监控。



## 时序数据库

时序数据库（Time Series Database，TSDB）是优化用于摄取、处理和存储时间戳数据的数据库，对比其他 SQL 和 NoSQL，其中时序数据库是专门用于存储和处理时间序列数据的数据库，支持时序数据高效读写、高压缩存储、插值和聚合等功能。

> InfluxDB，Prometheus 等都是比较常见时序数据库，其用于监控领域比较常见。

时序数据库以时间（时间点或者时间区间）来建立索引的数据库时序数据库被允许可以做出激进的存储、访问和保留策略（Retention Policies）：

- 日志结构的合并树（Log Structured Merge Tree，LSM-Tree）代替传统关系型数据库中的 B+ Tree 作为存储结构，LSM 适合的应用场景就是写多读少，且几乎不删改的数据。
- 设置激进的数据保留策略，比如根据过期时间（TTL），自动删除数据以节省存储空间，同时提高查询性能。
- 对数据进行再采样（Resampling）以节省空间（比如最近几天的数据可能需要精确到秒，而查询一个月前的数据只需要精确到天，查询一年前的数据只需要精确到周），将数据重新采样汇总，极大地节省了存储空间。

### LSM 树

LSM 树（Log-Structured Merge Tree）牺牲了一定的读性能换取写入数据的高性能。

> Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎。

数据写入到 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 Key 来排序的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 Write Ahead Log 的方式将数据备份在磁盘上。

MemTable 在累积到一定规模时，会被刷新生成一个新的文件 SSTable（Sorted String Table）。当 SSTable 达到一定数量时这些 SSTable 合并减少文件的数量，因为 SSTable 都是有序的，所以合并的速度很快。

当从 LSM 树里面读数据时，我们首先从 MemTable 中查找数据，如果数据没有找到，再从 SSTable 中查找数据。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引。




