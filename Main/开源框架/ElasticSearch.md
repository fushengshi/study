# ElasticSearch

ElasticSearch 是一款非常强大的、基于 Lucene 的开源搜索及分析引擎。除了搜索，结合 Kibana、Logstash、Beats 开源产品，Elastic Stack（简称 ELK）还被广泛运用在大数据近实时分析领域，包括：日志分析、指标监控、信息安全等。

Elasticsearch 也是使用 Java 编写的，它的内部使用 Lucene 做索引与搜索，但是它的目的是使全文检索变得简单，通过隐藏 Lucene 的复杂性，取而代之的提供一套简单一致的 RESTful API。

![image-20240731222026609](https://gitee.com/fushengshi/image/raw/master/image-20240731222026609.png)

- Cluster 集群

  一个集群由一个唯一的名字标识（默认为 elasticsearch）。具有相同集群名的节点才会组成一个集群。集群名称可以在配置文件中指定。

- Node 节点

  存储集群的数据，参与集群的索引和搜索功能。

- Index 索引

  一个索引是一个文档的集合。一个集群中可以有任意多个索引。

  > 类似数据库的表。

- Document 文档

  被索引的一条数据，索引的基本信息单元，以 JSON 格式来表示。

  > 类似数据库的一行数据。

- Shard 分片

  一个分片相当于一个 Lucene 中的 Lucene index。

  在创建一个索引时可以指定分成多少个分片来存储。每个分片本身也是一个功能完善且独立的索引，可以被放置在集群的任意节点上。

- Segment 分段

  Lucene index 由多个 Segment（段文件，倒排索引）组成。

- Replication 备份

  一个分片可以有多个备份（副本）。

集群五大角色：

- Master Node 主节点

  主节点不和应用创建连接，维护集群状态，控制整个集群的元数据。

  集群状态信息，只由 Master 节点进行维护，并且同步到集群中所有节点，其他节点只负责接收从 Master 同步过来的集群信息而没有维护的权利。

  只有 Master Node 节点可以修改节点状态信息及元数据（metadata），比如索引的新增、删除、分片路由分配、所有索引和相关 Mapping 、Setting 配置等等。

- Master eligible nodes 合格主节点

  有资格成为 Master 节点但暂时并不是 Master 的节点被称为 eligible 节点，该节点可以参加选主流程，成为 Master 节点。该节点只是与集群保持心跳，判断 Master 是否存活，如果 Master 故障则参加新一轮的 Master 选举。

  > 每个节点部署后不修改配置信息，默认就是一个 eligible 节点。

- Data Node 数据节点

  用于建立文档索引，真正存储数据，接收应用创建连接、接收索引请求，接收用户的搜索请求。

- Coordinating Node 协调节点（路由节点、Client节点）

  专用与接收应用的查询连接、接受搜索请求，但其本身不负责存储数据。

- Ingest Node

  可以看作是数据前置处理转换的节点，支持 pipeline 管道设置，可以使用 Ingest 对数据进行过滤、转换等操作（类似于 logstash 中 filter 的作用）。

> 实时性：插入 ES 的数据不一定马上能查得到。
>
> 事务性、数据一致性：ES 没有 RDB 意义的事务概念，也不存在提交操作。即便是一个bulk请求提交的数据，每条数据的成功与否都是互相独立的，并不保证要成功都成功，要失败都失败。



## 倒排索引

倒排索引是一种用于全文搜索的数据结构，它将文档中的每个单词映射到包含该单词的所有文档的列表中，然后用该列表替换单词。具体来说，一个倒排索引包含一个词语词典和每个词语对应的倒排列表。倒排列表中记录了包含该词语的所有文档的编号、词频等信息。

倒排索引中，每个单词被视为一个 Term，每个 Term 都有一个对应的 Term ID，而每个文档则有一个对应的文档ID。对于每个 Term，倒排索引维护一个包含该 Term 的所有文档的列表（Posting List），每个 Posting List 中包含该 Term 在对应文档中出现的位置信息。



## 数据持久化

![image-20240810000419326](https://gitee.com/fushengshi/image/raw/master/image-20240810000419326.png)

数据写入过程：

- 数据写入内存缓存区和 Translog 日志文件

  当写一条数据 doc 的时候，写入到内存缓冲区同时写入到 Translog 日志文件中。

- `refresh` 写入 OS系统文件缓存区

  内存缓存区满了或者每隔 1 秒（默认1秒），通过 `refresh` 将内存缓存区（index Buffer）的数据生成 index segment 文件并写入 OS 系统文件缓存区，然后清空内存缓存区。

  > index Buffer 是 ES 内存的一部分。
  >
  > OS系统文件缓存区是操作系统的，不属于 ES 内存。数据只有到了 OS 系统文件缓存才能被搜索到，这个延迟也是 ES 被称为**近实时搜索**的原因。

  由于 `refresh` 默认间隔为 1s，会产生大量的小 Segment，为此 ES 会运行一个任务检测当前磁盘中的 Segment，对符合条件的 Segment 进行合并操作（段合并），减少 Lucene 中的 Segment 个数，提高查询速度，降低负载。

- `flush` OS系统文件缓存区刷盘

  Translog 日志文件长度达到一定程度，或者默认每隔 30 分钟，会触发 `flush` 操作。

  > `flush` 完成了 Lucene 的 commit 操作，完成 Lucene 持久化。

  - 将内存缓存区中现有数据 `refresh` 到 OS系统文件缓存区中，清空内存缓存区。
  - 将一个 commit point 写入磁盘文件，同时强行将文件缓存区中所有的数据 `fsync` 到磁盘文件。
  - 清空 Translog 日志文件并重建。



## 数据可靠性

- Translog 日志

  当一个文档写入 Lucene 后是存储在内存中的，即使执行了 `refresh` 操作仍然是在文件系统缓存中，如果此时服务器宕机，这部分数据将会丢失。

  为此进行文档写操作时，先将文档写入 Lucene（内存缓冲区），然后写入一份到 Translog，防止服务器宕机后数据的丢失。

  - 同步刷盘（默认）
  
    每次修改操作完成后立刻执行 `fsync` 命令刷盘。
  
    只有在 Translog 被写入磁盘，ES 才会将操作成功的结果返回发送此操作请求的客户端。
  
  - 异步刷盘
  
    默认每 5s 执行 `fsync` 命令刷盘。
  
  > 与传统的分布式系统不同，这里是先写入 Lucene 再写入 Translog，原因是写入 Lucene 可能会失败，为了减少写入失败回滚的复杂度，因此先写入 Lucene。
  >
  > 因为只有在 Translog 被写入磁盘，ES 才会将操作成功的结果返回发送此操作请求的客户端，所以先写入 Lucene 再写入 Translog 对于可靠性没有影响。
  >
  > 写入 Translog 是顺序写，性能比较高。
  
- 多副本机制

  一个索引被分成多个分片，每个分片可以有一个主分片和多个副本分片，每个分片副本都是一个具有完整功能的 Lucene 实例。分片可以分配在不同的服务器上，同一个分片的不同副本不能分配在相同的服务器上。



## 高可用

### 集群 Master 选举

Master 选举的意义在于集群主节点在遭遇宕机时保障服务的可用性，ES 基于 Bully 和 Paxos 两种算法实现，ES 7.x 基于以上算法，加入了基于 Raft 的优化。

- Bully 算法：Bully 是 Leader 选举的基本算法之一，基本原理就是按照节点 ID 进行排序，任何时候当前 Leader 的节点 ID 都是集群中最高节点 ID。

- Paxos 算法

- Raft 算法

  ES 实现中，候选人不先投自己，而是直接并行发起 RequestVote，这相当于候选人有投票给其他候选人的机会。这样的好处是可以在一定程度上避免 3 个节点同时成为候选人时，都投自己，无法成功选主的情况。

  ES 不限制每个节点在某个 Term 上只能投一票， 节点可以投多票，这样会产生选出多个主的情况。对于这种情况，ES 的处理是让最后当选的 Leader 成功，作为 Leader。



### 副本故障恢复

ES 使用数据分片（Shard）来提高服务的可用性，将数据分散保存在不同的节点上以降低当单个节点发生故障时对数据完整性的影响，同时使用副本（Replica）来保证数据的完整性。

在索引写入时，副本分片做着与主分片相同的工作，新文档首先被索引进主分片然后再同步到其它所有的副本分片。增加副本数并不会增加索引容量，但是可以提升查询能力。

如果持有主分片的节点宕机，一个副本分片就会晋升为主分片。



