# 网络系统

## 阻塞 I/O

同步阻塞 I/O 模型中，应用程序发起 `read()` 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。

- 服务端通过 `socket()` 函数陷入内核态进行 Socket 系统调用，该内核函数会创建 **Socket 内核对象**，主要有两个重要的结构体进程等待队列和数据接收队列。

  - 进程等待队列，存放了服务端的用户进的进程描述符和回调函数。
  - 数据接收队列，存放网卡接收到的该 Socket 要处理的数据。

- 进程 `recvfrom()` 系统调用函数，发现 Socket 的数据等待队列没有它要接收的数据到达时，进程会让出 CPU 进入阻塞状态。

  进程的进程描述符和它被唤醒用到的回调函数 `callback func` 会组成一个结构体叫等待队列项，放入 **Socket 进程等待队列**。

- 网卡获取网络传输过来的数据：

  - 通过 DMA 控制程序复制到内存环形缓冲区 RingBuffer 中。
  - 网卡向 CPU 发出**硬中断**。

- CPU 收到了硬中断后：

  - 进行简单快速处理后向内核中断进程 `ksoftirqd` 发出**软中断**。
  - 释放 CPU，由软中断进程处理复杂耗时的网络设备请求逻辑。

- 内核中断进程 `ksoftirqd` 收到软中断信号后：

  - 将网卡复制到内存的数据，根据数据报文的 IP 和端口号，将其拷贝到对应 Socket 的接收队列。
  - 根据 Socket 的数据接收队列的数据，通过进程等待队列中的回调函数，唤醒要处理该数据的进程，进程会进入 CPU 的运行队列，等待获取 CPU 执行数据处理逻辑。

- 进程获取 CPU 后，会回到之前调用 `recvfrom()` 函数时阻塞的位置继续执行，发现 Socket 内核空间的等待队列上有数据，会在内核态将内核空间的 Socket 等待队列的数据拷贝到用户空间，然后回到用户态执行进程的用户程序，解除阻塞。



## I/O 多路复用

### Socket 模型

在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：

- TCP 半连接队列

  一个是还没完全建立连接的队列，这个队列都是没有完成三次握手的连接，此时服务端处于 `syn_rcvd` 的状态。

- TCP 全连接队列

  一个是已经建立连接的队列，这个队列都是完成了三次握手的连接，此时服务端处于 `established` 状态。

当 TCP 全连接队列不为空后，服务端的 `accept()` 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。



### 文件描述符

文件描述符（file descriptor）是一个非负整数，从 0 开始。进程使用文件描述符来标识一个打开的文件。Linux 中一切皆文件。

系统为每一个进程维护了一个文件描述符表，表示该进程打开文件的记录表，而**文件描述符实际上就是这张表的索引**。每个进程默认都有 3 个文件描述符：0 (stdin)、1 (stdout)、2 (stderr)。



### select

select 实现多路复用的方式是，将已连接的 Socket 都放到一个文件描述符集合，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生。

- 通过遍历文件描述符集合的方式，当检查到有事件后，将此 Socket 标记为可读或可写。
- 把整个文件描述符集合拷贝回用户态。
- 用户态通过遍历的方法找到可读或可写的 Socket 对其进行处理。

整个过程：

- 两次遍历文件描述符集合

  一次是在内核态里，一个次是在用户态里。

- 两次拷贝文件描述符集合

  先从用户空间拷贝到内核空间，由内核修改后，再拷贝到用户空间中。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，Linux 系统由内核中的 FD_SETSIZE 限制， 默认最大值为 1024，只能监听 0~1023 的文件描述符。

### poll

```c
struct pollfd {
    int fd;           // 要监听的文件描述符
    short events;     // 要监听的事件
    short revents;    // 文件描述符fd上实际发生的事件
};
```

poll 用动态数组取代 BitsMap 存储关注的文件描述符，以链表形式来组织，突破了 select 的文件描述符个数限制（会受到系统文件描述符限制）。

poll 和 select 并没有太大的本质区别：

- 使用线性结构存储进程关注的 Socket 集合，都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)。
- 在用户态与内核态之间拷贝文件描述符集合，随着并发数增加性能的损耗呈指数级增长。



### epoll

```c
int epoll_create(int size); // 创建一个eventpoll内核对象

int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // 将连接到Socket对象添加到eventpoll对象上，epoll_event是要监听的事件

int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); // 等待连接Socket的数据是否到达
```

#### epoll_create 

`epoll_create` 函数会创建一个 struct eventpoll 的内核对象，把它关联到当前进程的已打开文件列表中。

```c
struct eventpoll {
    wait_queue_head_t wq;      // 进程等待队列，存放阻塞的进程

    struct list_head rdllist;  // 就绪队列，数据就绪的文件描述符都会放到这里

    struct rb_root rbr;        // 红黑树，管理用户进程下添加进来的所有 socket 连接
    ......
}
```

![image-20240504193052358](https://gitee.com/fushengshi/image/raw/master/image-20240504193052358.png)





#### epoll_ctl

`epoll_ctl` 函数主要负责把服务端和客户端建立的 Socket 连接注册到 eventpoll 对象里：

- 创建一个 epitem 对象

  主要包含两个字段，分别存放 Socket fd （连接的文件描述符）、所属的 eventpoll 对象的指针。

- 将数据到达时用到的回调函数 `ep_poll_callback` 添加到 **Socket 进程等待队列**中。

  > 与阻塞 I/O 模式不同的是，这里添加的 Socket 的进程等待队列结构中，只有回调函数，没有设置进程描述符。
  >
  > 在 epoll 中，进程是放在 eventpoll 的等待队列中，等待被 `epoll_wait` 函数唤醒，而不是放在 Socket 的进程等待队列中。

- 将创建的 epitem 对象插入红黑树。



#### epoll_wait

检查 eventpoll 对象的就绪的连接 rdllist 上是否有数据到达，如果没有就把当前的进程描述符添加到一个等待队列项并加入到 eventpoll 的进程等待队列，然后阻塞当前进程，等待数据到达时通过回调函数被唤醒。

- 触发 Socket 进程等待队列回调函数
  
  Socket 的数据接收队列有数据到达，通过 **Socket 进程等待队列**的 `ep_poll_callback` 回调函数：

  - 添加就绪队列
  
    `ep_poll_callback` 函数将有数据到达的 epitem 添加到 eventpoll 对象的**就绪队列** rdllist 中。
  
  - 唤醒进程等待队列
  
    `ep_poll_callback` 函数检查 eventpoll 对象的**进程等待队列**上是否有等待项，通过回调函数 `default_wake_func` 唤醒这个进程，进行数据的处理。
  
- 当进程醒后从 `epoll_wait` 时暂停的代码继续执行，把 rdlist 中就绪的事件返回给用户进程，用户进程调用 `recv` 把已经到达内核 Socket 等待队列的数据拷贝到用户空间使用。



epoll 通过两个方面解决了 select/poll 的问题：

- epoll 在内核里使用红黑树跟踪进程所有待检测的 Socket

  需要监控的 Socket 通过 `epoll_ctl` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改时间复杂度是 O(logn)。

  epoll 因为在内核维护了红黑树，可以保存所有待检测的 Socket ，所以 `epoll_ctl` 只需要传入一个待检测的 Socket，减少了内核和用户空间大量的数据拷贝和内存分配。

- epoll 使用**事件驱动**的机制

  内核里维护了一个链表来记录就绪事件，当 Socket 有事件发生时，内核会通过回调函数将其加入到就绪事件列表中。

  用户调用 `epoll_wait` 函数时，只会返回有事件发生的文件描述符的个数，不需要 select/poll 那样轮询扫描整个 Socket 集合，大大提高了检测的效率。

> 没有使用共享内存。`epoll_wait` 实现的内核代码中调用了 `__put_user` 函数，这个函数就是将数据从内核拷贝到用户空间。



###  边缘触发和水平触发

epoll 支持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）。

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 `epoll_wait` 中苏醒一次，即使进程没有调用 `read()` 函数从内核读取数据，也只苏醒一次。

  程序要保证一次性将内核缓冲区的数据读取完。

- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，服务器端不断地从 `epoll_wait` 中苏醒，直到内核缓冲区数据被 `read()` 函数读完才结束，目的是告诉有数据需要读取。



## 异步 I/O 

异步 I/O（Asynchronous I/O），内核会等待数据准备完成，然后将数据拷贝到用户内存，一切都完成之后，内核会给用户进程发送信号通知操作完成。

> 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了，因为 Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。



## 零拷贝

> Kafka 开源项目利用了零拷贝技术，大幅提升 I/O 吞吐率。
>
> 调用了 Java NIO 库里的 `transferTo()` 方法，Linux 系统支持 `sendfile` 系统调用， `transferTo()` 实际会使用到 `sendfile` 系统调用函数。

### DMA技术

直接内存访问（Direct Memory Access） 技术，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器， CPU 不再参与任何与数据搬运相关的事情，可以去处理别的事务。

### read + write

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240529230740028.png" alt="image-20240529230740028" style="zoom: 67%;" />

- 发生了四次用户态与内核态的上下文切换

  发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都要先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。

- 发生了四次数据拷贝

  其中两次是 DMA 的拷贝，另外两次是 CPU 的拷贝：

  - 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程通过 DMA 搬运。
  - 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，应用程序可以使用这部分数据，这个拷贝到过程由 CPU 完成。
  - 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 Socket 的缓冲区里，这个过程由 CPU 搬运。
  - 第四次拷贝，把内核的 Socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程由 DMA 搬运。



### mmap + write

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240529230815966.png" alt="image-20240529230815966" style="zoom:67%;" />

- 4 次上下文切换

  系统调用还是 2 次，一次是 `mmap()` ，一次是 `write()`。

- 3 次数据拷贝

  2 次 DMA 拷贝，1 次 CPU 拷贝。

  通过 CPU 把内核缓冲区的数据拷贝到 Socket 缓冲区里，系统调用函数 `mmap()` 会直接把内核缓冲区里的数据映射到用户空间，操作系统内核与用户空间不需要进行数据拷贝操作。

  > `mmap` 是一种内存映射文件的方法，将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。
  >
  > 进程可以采用指针的方式读写操作这一段内存，系统会自动回写脏页面到对应的文件磁盘上，对文件的操作不必再调用 `read` 和 `write` 等系统调用函数。



### sendfile

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240529230928345.png" alt="image-20240529230928345" style="zoom:67%;" />

- 2 次上下文切换

  一次系统调用 `sendfile()` ，系可以直接把内核缓冲区里的数据拷贝到 Socket 缓冲区里，不再拷贝到用户态。

- 3 次数据拷贝

  2 次 DMA 拷贝，1 次 CPU 拷贝。

> FileChannel 的 `transferTo()/transferFrom()` 是基于发送文件 `sendfile` 这种零拷贝方式的提供的一种实现，底层实际是调用了 Linux 内核的 `sendfile` 系统调用。可以直接将文件数据从磁盘发送到网络，而不需要经过用户空间的缓冲区。
>

### sendfile（SG-DMA）

如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

<img src="https://gitee.com/fushengshi/image/raw/master/image-20240529230958933.png" alt="image-20240529230958933" style="zoom:67%;" />

- 2 次上下文切换

  系统调用函数 `sendfile()` 直接把内核缓冲区里的数据拷贝到 Socket 缓冲区里，不再拷贝到用户态。

- 2 次数据拷贝

  1 次 DMA 拷贝，1 次 SG-DMA 拷贝。

  对于支持网卡支持 SG-DMA 技术的情况下， `sendfile()` 系统调用的过程发生了点变化，具体过程：

  - 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里。
  - 第二步，缓冲区描述符和数据长度传到 Socket 缓冲区，网卡的 SG-DMA 控制器直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 Socket 缓冲区中，减少了一次数据拷贝。

  只需要两次上下文切换和数据拷贝次数，就可以完成文件的传输，而且两次的数据拷贝过程，都不需要通过 CPU，两次都是由 DMA 完成。

> 传输文件的时候，根据文件的大小来使用不同的方式：
>
> - 传输大文件的时候，使用异步 I/O + 直接 I/O。
>
>   > 详见 Page Cache。
>
> - 传输小文件的时候，使用零拷贝技术。
>




## 高性能网络模式

### Reactor 模式

Reactor 模式也叫 Dispatcher 模式，即 I/O 多路复用监听事件，收到事件后根据事件类型分配（Dispatch）给某个进程 / 线程。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件。
- 处理资源池负责处理事件。

常见的 Reactor 实现方案有三种：

- 单 Reactor 单进程 / 线程

  不用考虑进程间通信以及数据同步的问题，实现比较简单，方案的缺陷在于无法充分利用多核 CPU，处理业务逻辑的时间不能太长，否则会延迟响应。

  不适用于计算机密集型的场景，适用于业务处理快速的场景。

  > Redis 采用单 Reactor 单进程的方案。

- 单 Reactor 多线程

  只有一个 Reactor 对象来承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方。

- 多 Reactor 多进程 / 线程

  主 Reactor 只负责监听事件，响应事件的工作交给了从 Reactor。
  
  > Netty 和 Memcache 都采用了多 Reactor 多线程方案。
  >
  > Nginx 则采用了类似于多 Reactor 多进程方案。



### Proactor 模式

Reactor 可以理解为来了事件操作系统通知应用进程，让应用进程来处理，而 Proactor 可以理解为来了事件操作系统来处理，处理完再通知应用进程。

无论是 Reactor还是 Proactor，都是一种基于事件分发的网络编程模式，区别在于 Reactor 模式是基于待完成的 I/O 事件，而 Proactor 模式则是基于已完成的 I/O 事件。



# 附录

## Java NIO 示例

```java
public class MyNIOServer {
    public static void main(String[] args) throws IOException {
        // 开启服务端Socket
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        serverSocketChannel.bind(new InetSocketAddress(5555));
        // 设置为非阻塞
        serverSocketChannel.configureBlocking(false);
        // 开启一个选择器
        Selector selector = Selector.open();
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);
        while (true) {
            // 阻塞等待需要处理的事件发生
            selector.select();
            // 获取selector中注册的全部事件的 SelectionKey 实例
            Set<SelectionKey> selectionKeys = selector.selectedKeys();
            // 获取已经准备完成的key
            Iterator<SelectionKey> iterator = selectionKeys.iterator();
            while (iterator.hasNext()) {
                SelectionKey next = iterator.next();
                // 当发现连接事件
                if (next.isAcceptable()) {
                    // 获取客户端连接
                    SocketChannel socketChannel = serverSocketChannel.accept();
                    // 设置非阻塞
                    socketChannel.configureBlocking(false);
                    // 将该客户端连接注册进选择器 并关注读事件
                    socketChannel.register(selector, SelectionKey.OP_READ);
                    System.out.println("accepted connection from " + socketChannel);
                    // 如果是读事件
                } else if (next.isReadable()) {
                    ByteBuffer allocate = ByteBuffer.allocate(128);
                    // 获取与此key唯一绑定的channel
                    SocketChannel channel = (SocketChannel) next.channel();
                    // 开始读取数据
                    int read = channel.read(allocate);
                    if (read > 0) {
                        System.out.println(new String(allocate.array()));
                    } else if (read == -1) {
                        System.out.println("断开连接");
                        channel.close();
                    }
                }
                // 删除这个事件
                iterator.remove();
            }
        }
    }
}
```



## Java NIO 源码

Java 中的 NIO（Non-blocking / New I/O）可以看作是 I/O 多路复用模型。

> 不同平台的实现不同，Linux 2.6 之前是select、poll，2.6 之后是 **epoll**，Windows是 IOCP。

- `epoll_create`

  对应Java NIO代码中的 `Selector.open()`。

- `epoll_ctl`

  ~~对应 Java NIO 代码中的 `socketChannel.register(selector, xxx)`~~。

  源码分析 `epoll_ctl` 和 `epoll_wait` 都在 `selector.select()` 中执行。

- `epoll_wait`

  对应 Java NIO 代码中的 `selector.select()`。

### Selector.open()

创建 Selector。

```java
public static Selector open() throws IOException {
    // 没有做特殊配置，会使用默认的DefaultSelectorProvider创建SelectorProvider。
    // 不同平台的 DefaultSelectorProvider 实现不同。
    return SelectorProvider.provider().openSelector();
}

// Linux
public class DefaultSelectorProvider {
    private static final SelectorProviderImpl INSTANCE;
    static {
        PrivilegedAction<SelectorProviderImpl> pa = EPollSelectorProvider::new;
        INSTANCE = AccessController.doPrivileged(pa);
    }
}

public class EPollSelectorProvider extends SelectorProviderImpl {
    public AbstractSelector openSelector() throws IOException {
        return new EPollSelectorImpl(this);
    }
}

class EPollSelectorImpl extends SelectorImpl {
    EPollSelectorImpl(SelectorProvider sp) throws IOException {
        super(sp);

        // 本地方法，执行 epoll_create。
        // static native int create() throws IOException;
        this.epfd = EPoll.create();
        this.pollArrayAddress = EPoll.allocatePollArray(NUM_EPOLLEVENTS);

        try {
            this.eventfd = new EventFD();
            IOUtil.configureBlocking(IOUtil.newFD(eventfd.efd()), false);
        } catch (IOException ioe) {
            EPoll.freePollArray(pollArrayAddress);
            FileDispatcherImpl.closeIntFD(epfd);
            throw ioe;
        }

        // register the eventfd object for wakeups
        EPoll.ctl(epfd, EPOLL_CTL_ADD, eventfd.efd(), EPOLLIN);
    }
}
```

### socketChannel.register()

将需要监控的通道注册到 Selector 中，AbstractSelectableChannel 中实现。

当新的通道向 Selector 注册时会创建一个 SelectionKey，并将其保存到 `SelectionKey[] keys` 缓存中。新的 SelectionKey 会调用到`AbstractSelector.register()`，首先会先创建一个 SelectionKeyImpl，然后调用方法 `implRegister()` 执行实际注册。

```java
public abstract class SelectorImpl extends AbstractSelector {
    protected final SelectionKey register(AbstractSelectableChannel ch, int ops, Object attachment) {
        if (!(ch instanceof SelChImpl))
            throw new IllegalSelectorException();
        SelectionKeyImpl k = new SelectionKeyImpl((SelChImpl)ch, this);
        k.attach(attachment);
        synchronized (publicKeys) {
            // 在各个平台的 SelectorImpl 的实现类中具体实现。
            implRegister(k);
        }
        // 添加到selector
        k.interestOps(ops);
        return k;
    }

    @Override
    public SelectionKey interestOps(int ops) {
        ensureValid();
        if ((ops & ~channel().validOps()) != 0)
            throw new IllegalArgumentException();
        int oldOps = (int) INTERESTOPS.getAndSet(this, ops);
        if (ops != oldOps) {
            // 将SelectionKey自身添加到selector
            selector.setEventOps(this);
        }
        return this;
    }
}

// Linux
class EPollSelectorImpl extends SelectorImpl {
    // 没有实现implRegister方法
    @Override
    public void setEventOps(SelectionKeyImpl ski) {
        synchronized (updateLock) {
            // 加入到队列，在doSelect中调用
            updateKeys.addLast(ski);
        }
    }
}
```

### selector.select()

```java
@Override
protected int doSelect(Consumer<SelectionKey> action, long timeout)
    throws IOException {
    assert Thread.holdsLock(this);

    int to = (int) Math.min(timeout, Integer.MAX_VALUE);
    boolean blocking = (to != 0);
    boolean timedPoll = (to > 0);

    int numEntries;
    // 从updateKeys队列获取selectionKey，执行本地方法epoll_ctl
    processUpdateQueue();
    processDeregisterQueue();
    try {
        begin(blocking);

        do {
            long startTime = timedPoll ? System.nanoTime() : 0;
            boolean attempted = Blocker.begin(blocking);
            try {
                // 本地方法epoll_wait
                numEntries = EPoll.wait(epfd, pollArrayAddress, NUM_EPOLLEVENTS, to);
            } finally {
                Blocker.end(attempted);
            }
            if (numEntries == IOStatus.INTERRUPTED && timedPoll) {
                long adjust = System.nanoTime() - startTime;
                to -= (int) TimeUnit.NANOSECONDS.toMillis(adjust);
                if (to <= 0) {
                    numEntries = 0;
                }
            }
        } while (numEntries == IOStatus.INTERRUPTED);
        assert IOStatus.check(numEntries);
    } finally {
        end(blocking);
    }
    processDeregisterQueue();
    return processEvents(numEntries, action);
}
```



## MappedByteBuffer

MappedByteBuffer 是基于内存映射 `mmap` 零拷⻉⽅式的⼀种实现，底层实际是调用了 Linux 内核的 `mmap` 系统调用。可以将一个**文件**或者文件的一部分映射到内存中，形成一个虚拟内存文件，可以直接操作内存中的数据，不需要通过系统调用来读写文件。

![image-20240510003554918](https://gitee.com/fushengshi/image/raw/master/image-20240510003554918.png)

> DirectByteBuffer（MappedByteBuffer 子类）在堆外内存开辟位置，**省略堆外内存到堆内存的拷贝过程**。
>
> 利用 `mmap` 内存映射方式将堆外的部分内存和内核空间的部分内核进行映射，可以视为使用了同一个物理内存地址，可以**省略内核缓冲区到堆外内存拷贝的过程**。

### mmap

`mmap` 是一种**内存映射文件**的方法，将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。

进程通过对虚拟地址（指针）读写的方式实现对内存映射区对应的物理内存的读写，读写会被系统自动通过后台线程 flusher 刷新到存储空间、或者通过 `msync` 调用刷新到存储空间，不用调用 `read` 和 `write` 等系统调用实现对文件的读写。

![image-20240721152634553](https://gitee.com/fushengshi/image/raw/master/image-20240721152634553.png) 


- 创建虚拟映射区域

  由内核在当前进程的虚拟地址空间中，寻找能够满足映射长度需求的连续虚拟地址区间，为连续虚拟地址区间分配一个vm_area_struct 结构体并初始化成员变量。

  ```c
  struct vm_area_struct {
      // MAP_SHARED 共享映射
      unsigned long vm_flags; 
  }
  ```

  将新建的虚拟区间结构体插入到当前进程的 vm_area_struct 结构体链表和红黑树结构中。

- 内核空间 `mmap`

  建立页表，实现文件物理地址和虚拟地址区域的映射关系。此时虚拟地址并没有任何数据关联到主存中，只是建立了虚拟地址和文件物理地址之间的映射关系。

- 进程访问

  - 进程的读或者写操作访问虚拟地址区间中的一个或者一段映射地址，通过查询页表发现主存上这一段地址不存在对应的物理页面，引发**缺页异常**。

  - 缺页异常通过一系列判断，确定操作合法后，通过 DMA 的方式读取数据。

  - 调页过程先在交换缓存空间（swap cache）中寻找需要访问的内存页，如果没有则把所缺的页文件内容拷贝到物理内存（主存）。

    > 如果文件页不在 Page Cache 中，内核会在物理内存中分配一个内存页，然后将新分配的内存页加入到 Page Cache 中。
    
    内核会为映射的虚拟内存在页表中创建 PTE（Page Table Entry），然后将虚拟内存与 Page Cache 中的内存页通过 PTE 关联。当一个页面被写入，CPU 会在这个页面对应的 PTE 中设置一个 dirty 标志位来表示对应的页面内容被修改。

  - 进程从缺页异常中恢复即可读取该内容文件对应的实际内容或者对文件进行写操作。

![image-20240721173248343](https://gitee.com/fushengshi/image/raw/master/image-20240721173248343.png)

### 共享文件映射

![image-20240721160938516](https://gitee.com/fushengshi/image/raw/master/image-20240721160938516.png)

多个进程中的虚拟内存映射区最终会通过缺页中断的方式映射到文件的 Page Cache 中，后续多个进程对各自的这段虚拟内存区域的读写都会直接发生在 page cache 上。

因为映射文件的 Page Cache 在内核中只有一份，所以对于共享文件映射来说，多进程读写都是共享的，由于多进程直接读写的是 Page Cache ，所以多进程对共享映射区的任何修改，最终都会通过内核回写线程 `pdflush` 
刷新到磁盘文件中。











